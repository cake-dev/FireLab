{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire Severity Prediction using Raster Tools and SciKit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REQUIRED SOFTWARE\n",
    "- Python\n",
    "    - raster-tools\n",
    "    - scikit-learn\n",
    "    - tqdm\n",
    "    - numpy\n",
    "    - dask\n",
    "    - geopandas\n",
    "    - dask_geopandas\n",
    "    - joblib\n",
    "    - ray\n",
    "\n",
    "- Command Line\n",
    "    - gdal\n",
    "    - cdo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is recommended to have about 300GB of free space on your drive to complete the Wildfire Severity Prediction process. (may need to be done in chunks if not enough space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data to run the script is obtained from multiple sources.  The table below shows the data sources and links to where the data can be found.\n",
    "\n",
    "<table><th>Source</th><th>Link</th><th>Description</th></tr>\n",
    "<tr><td>MTBS Fire Data</td><td>https://www.mtbs.gov/direct-download</td><td>Fire Bundles -> Burned Areas Boundaries & Burn Severity Mosaics -> 1986-2020 of desired state</td></tr>\n",
    "<tr><td>DEM Data</td><td>https://earthexplorer.usgs.gov/</td><td>Data sets -> Digital elevation -> CONUS aspect, flow_acc, orig_dem, slope</td></tr>\n",
    "<tr><td>gridMET Climate Data</td><td>https://www.climatologylab.org/gridmet.html</td><td>use the provided scripts</td></tr>\n",
    "<tr><td>AdaptWest Climate Data</td><td>https://adaptwest.databasin.org/pages/adaptwest-climatena/</td><td>Climate Normals -> 1991-2020 period -> 33 Bioclimatic variables zip</td></tr>\n",
    "<tr><td>DayMet Climate Data</td><td>https://daac.ornl.gov/cgi-bin/dataset_lister.pl?p=32</td><td>use the provided scripts</td></tr>\n",
    "<tr><td>Landfire Data</td><td>https://www.landfire.gov/version_download.php</td><td>LF 2016 Remap -> Fuel Veg Type 2020 & 40 Scott/Burgan Fuel Models 2020</td></tr>\n",
    "<tr><td>Biomass Data</td><td>https://rangelands.app/products/</td><td>use the provided scripts</td></tr>\n",
    "<tr><td>NDVI Data</td><td>https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/</td><td>need 1986-2020, either manually or using the provided scripts</td></tr>\n",
    "<tr><td>State Borders</td><td>https://www2.census.gov/geo/tiger/GENZ2018/shp/</td><td>need cb_2018_us_state_5m.zip</td></tr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are scripted methods available to obtain some the data.  They are as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biomass download and processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Set the desired area of interest\n",
    "# Change the values below to match the desired area of interest (i reccommend something like http://bboxfinder.com/ to get the coordinates)\n",
    "STATE = \"Oregon\"\n",
    "LONG_MIN = -124.85\n",
    "LONG_MAX = -116.33\n",
    "LAT_MIN = 41.86\n",
    "LAT_MAX = 46.23\n",
    "\n",
    "# Loop through years 2020 to 1986 to download biomass data\n",
    "years = list(range(2020, 1985, -1))\n",
    "\n",
    "for year in years:\n",
    "    # Download biomass data\n",
    "    subprocess.run([\"gdal_translate\", \"-co\", \"compress=lzw\", \"-co\", \"tiled=yes\", \"-co\", \"bigtiff=yes\",\n",
    "                    f\"/vsicurl/http://rangeland.ntsg.umt.edu/data/rap/rap-vegetation-biomass/v3/vegetation-biomass-v3-{year}.tif\",\n",
    "                    \"-projwin\", str(LONG_MIN), str(LAT_MAX), str(LONG_MAX), str(LAT_MIN),\n",
    "                    f\"out{year}_{STATE}.tif\"], check=True)\n",
    "\n",
    "    # Convert to netCDF format\n",
    "    subprocess.run([\"gdal_translate\", \"-of\", \"netCDF\", \"-co\", \"FORMAT=NC4\",\n",
    "                    f\"out{year}_{STATE}.tif\", f\"{year}_biomass_{STATE}.nc\"], check=True)\n",
    "\n",
    "    # Remove temporary files\n",
    "    subprocess.run([\"rm\", \"*.tif\"], check=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the biomass netCDF files are ready, the dates need to be fixed. Save and run the following script to fix the dates and combine the files into one netCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set the desired state\n",
    "STATE = \"OR\"\n",
    "\n",
    "# Loop through years 2020 to 1986\n",
    "years = list(range(2020, 1985, -1))\n",
    "\n",
    "for year in years:\n",
    "    if year % 4 == 0:\n",
    "        year_days = 366\n",
    "    else:\n",
    "        year_days = 365\n",
    "\n",
    "    # Fix dates and bands of biomass data\n",
    "    subprocess.run([\"cdo\", \"settaxis,{}-01-01,00:00,{}days\".format(year, year_days),\n",
    "                    \"{}_biomass.nc\".format(year),\n",
    "                    \"{}_biomass_fixed.nc\".format(year)], check=True)\n",
    "\n",
    "    # Split the data into individual bands\n",
    "    subprocess.run([\"cdo\", \"splitvar\",\n",
    "                    \"{}_biomass_fixed.nc\".format(year),\n",
    "                    \"{}_{}_biomass_\".format(STATE, year)], check=True)\n",
    "\n",
    "    # Remove temporary files\n",
    "    os.remove(\"{}_biomass.nc\".format(year))\n",
    "    os.remove(\"{}_biomass_fixed.nc\".format(year))\n",
    "\n",
    "# Create directories for bands\n",
    "os.makedirs(\"b1\", exist_ok=True)\n",
    "os.makedirs(\"b2\", exist_ok=True)\n",
    "\n",
    "# Move band files to respective directories\n",
    "os.rename(\"*Band1.nc\", \"b1/\")\n",
    "os.rename(\"*Band2.nc\", \"b2/\")\n",
    "\n",
    "# Change directory to b1\n",
    "os.chdir(\"b1\")\n",
    "\n",
    "# Concatenate band files\n",
    "subprocess.run([\"cdo\", \"-f\", \"nc4\", \"-z\", \"zip\", \"cat\", \"*.nc\",\n",
    "                \"biomass_afg_1986_2020_{}.nc\".format(STATE)], check=True)\n",
    "\n",
    "# Change directory to b2\n",
    "os.chdir(\"../b2\")\n",
    "\n",
    "# Concatenate band files\n",
    "subprocess.run([\"cdo\", \"-f\", \"nc4\", \"-z\", \"zip\", \"cat\", \"*.nc\",\n",
    "                \"biomass_pfg_1986_2020_{}.nc\".format(STATE)], check=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gridMET download and processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Loop through years 2020 to 1986\n",
    "years = list(range(2020, 1985, -1))\n",
    "\n",
    "for year in years:\n",
    "    # Download vpd, srad, and pdsi data for each year\n",
    "    subprocess.run([\"wget\", \"-nc\", \"-c\", \"-nd\",\n",
    "                    f\"http://www.northwestknowledge.net/metdata/data/vpd_{year}.nc\"], check=True)\n",
    "    subprocess.run([\"wget\", \"-nc\", \"-c\", \"-nd\",\n",
    "                    f\"http://www.northwestknowledge.net/metdata/data/srad_{year}.nc\"], check=True)\n",
    "    subprocess.run([\"wget\", \"-nc\", \"-c\", \"-nd\",\n",
    "                    f\"http://www.northwestknowledge.net/metdata/data/pdsi_{year}.nc\"], check=True)\n",
    "\n",
    "# Merge the downloaded data for each variable\n",
    "for var in [\"vpd\", \"srad\", \"pdsi\"]:\n",
    "    subprocess.run([\"cdo\", \"-f\", \"nc4\", \"-z\", \"zip\", \"cat\", f\"{var}_*.nc\",\n",
    "                    f\"{var}_1986-2020.nc\"], check=True)\n",
    "\n",
    "    # Calculate weekly mean for each variable\n",
    "    subprocess.run([\"cdo\", \"-f\", \"nc4\", \"-z\", \"zip\", \"-timselmean,7\",\n",
    "                    f\"{var}_1986-2020.nc\", f\"{var}_1986_2020_weekly.nc\"], check=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dayMET download and processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Loop through years 2020 to 1986\n",
    "years = list(range(2020, 1985, -1))\n",
    "\n",
    "for year in years:\n",
    "    # Download tmax and tmin data for each year\n",
    "    subprocess.run([\"wget\", \"-nc\", \"-c\", \"-nd\",\n",
    "                    f\"https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/2131/daymet_v4_tmax_monavg_na_{year}.nc\"],\n",
    "                   check=True)\n",
    "    subprocess.run([\"wget\", \"-nc\", \"-c\", \"-nd\",\n",
    "                    f\"https://thredds.daac.ornl.gov/thredds/fileServer/ornldaac/2131/daymet_v4_tmin_monavg_na_{year}.nc\"],\n",
    "                   check=True)\n",
    "\n",
    "# Merge the downloaded data for each variable\n",
    "for var in [\"tmax\", \"tmin\"]:\n",
    "    subprocess.run([\"cdo\", \"-f\", \"nc4\", \"-z\", \"zip\", \"cat\", f\"daymet_v4_{var}_monavg_na_*.nc\",\n",
    "                    f\"{var}_1986_2020.nc\"], check=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NDVI download and processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Loop through years 2020 to 1986\n",
    "years = list(range(2020, 1985, -1))\n",
    "\n",
    "for year in years:\n",
    "    # Download NDVI data for each year\n",
    "    subprocess.run([\"wget\", \"-erobots=off\", \"-nv\", \"-m\", \"-np\", \"-nH\", \"--cut-dirs=2\",\n",
    "                    \"--reject\", \"index.html*\", f\"https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/{year}/\"],\n",
    "                   check=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average and combine the NDVI data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Loop through years 2020 to 1986\n",
    "years = list(range(2020, 1985, -1))\n",
    "\n",
    "for year in years:\n",
    "    # Change directory to the current year\n",
    "    os.chdir(str(year))\n",
    "\n",
    "    # Concatenate daily NDVI files into a single yearly file\n",
    "    subprocess.run([\"cdo\", \"cat\", \"*.nc\", f\"{year}_ndvi_daily.nc\"], check=True)\n",
    "\n",
    "    # Calculate weekly average NDVI for each year\n",
    "    subprocess.run([\"cdo\", \"-f\", \"nc4\", \"-z\", \"zip\", \"-timselmean,7\",\n",
    "                    f\"{year}_ndvi_daily.nc\", f\"{year}_ndvi_weeklyavg.nc\"], check=True)\n",
    "\n",
    "    # Remove unnecessary variables from the weekly average file\n",
    "    subprocess.run([\"ncks\", \"-x\", \"-v\", \"TIMEOFDAY,QA\",\n",
    "                    f\"{year}_ndvi_weeklyavg.nc\", f\"{year}_ndvi_weeklyavg.nc\"], check=True)\n",
    "\n",
    "    # Move the weekly average file to the 'weekly' directory\n",
    "    os.rename(f\"{year}_ndvi_weeklyavg.nc\", \"../weekly/{year}_ndvi_weeklyavg.nc\")\n",
    "\n",
    "    # Change directory back to the parent directory\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# Change directory to the 'weekly' directory\n",
    "os.chdir(\"weekly\")\n",
    "\n",
    "# Concatenate all the yearly files into a single file\n",
    "subprocess.run([\"cdo\", \"cat\", \"*.nc\", \"ndvi_1986_2020_weeklyavg.nc\"], check=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The script used to build the dataset is located in the following path: Fire_Prediction/build_mtbs_dataframe.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once the data has been collected, the dataset can be built.  The files must be arranged in specific folders for the script to work.  The structure is as follows:\n",
    "\n",
    "<table><th>Feature</th><th>Folder</th></tr>\n",
    "<tr><td>MTBS Data</td><td>data/MTBS_Data</td></tr>\n",
    "<tr><td>DEM Data</td><td>data/terrain</td></tr>\n",
    "<tr><td>GridMet Climate Data</td><td>data/FeatureData/gridmet</td></tr>\n",
    "<tr><td>DayMet Climate Data</td><td>data/FeatureData/daymet</td></tr>\n",
    "<tr><td>AdaptWest Climate Data</td><td>data/FeatureData/adaptwest</td></tr>\n",
    "<tr><td>Landfire Fuel Data</td><td>data/FeatureData/landfire</td></tr>\n",
    "<tr><td>Biomass Data</td><td>data/FeatureData/biomass</td></tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data prepped, the folder structure in ```data/``` should look something like this:\n",
    "\n",
    "```\n",
    "├── FeatureData\n",
    "│   ├── adaptwest\n",
    "    │   ├── check1\n",
    "│   ├── biomass\n",
    "│   │   └── all_biomass\n",
    "│   │       ├── b1\n",
    "│   │       └── b2\n",
    "│   ├── daymet\n",
    "│   │   ├── tmax_monthavg\n",
    "│   │   └── tmin_monthavg\n",
    "│   ├── gridmet\n",
    "│   ├── landfire\n",
    "│   │   ├── LF2020_FBFM40_200_CONUS\n",
    "│   │   │   ├── ...\n",
    "│   │   └── LF2020_FVT_200_CONUS\n",
    "│   │       ├── ..\n",
    "│   ├── ndvi\n",
    "│   └── state_borders\n",
    "├── MTBS_Data\n",
    "│   ├── MTBS_BSmosaics\n",
    "│   │   ├── 1986\n",
    "│   │   ├── ...\n",
    "│   ├── mtbs_burn_mosaics\n",
    "│   └── mtbs_perimeter_data\n",
    "├── temp\n",
    "│   ├── check1\n",
    "│   ├── check2\n",
    "│   ├── ...\n",
    "└── terrain\n",
    "    ├── us_aspect\n",
    "    │   ├── ...\n",
    "    ├── us_flow_acc\n",
    "    │   ├── ...\n",
    "    ├── us_orig_dem\n",
    "    │   ├── ...\n",
    "    └── us_slope\n",
    "        ├── ...\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The script will generate a Dask dataframe in Parquet format.  The dataframe will have 21 columns, 20 of which are features and 1 of which is the target variable.  The features are as follows:\n",
    "\n",
    "<table><th>Feature</th><th>Column Name</th><th>Temporal Range</th></tr>\n",
    "<tr><td>MTBS Severity Rating</td><td>mtbs</td><td>Const</td></tr>\n",
    "<tr><td>MTBS Fire Year</td><td>year</td><td>Const</td></tr>\n",
    "<tr><td>DEM elevation</td><td>dem</td><td>Const</td></tr>\n",
    "<tr><td>DEM slope</td><td>dem_slope</td><td>Const</td></tr>\n",
    "<tr><td>DEM aspect</td><td>dem_aspect</td><td>Const</td></tr>\n",
    "<tr><td>DEM flow accumulation</td><td>dem_flow_acc</td><td>Const</td></tr>\n",
    "<tr><td>DEM hillshade</td><td>hillshade</td><td>Const</td></tr>\n",
    "<tr><td>GridMet Drought Index</td><td>gm_pdsi</td><td>Weekly avg</td></tr>\n",
    "<tr><td>GridMet Solar Radiation</td><td>gm_srad</td><td>Weekly avg</td></tr>\n",
    "<tr><td>GridMet Vapor Pressure</td><td>gm_vpd</td><td>Weekly avg</td></tr>\n",
    "<tr><td>DayMet Temp Max</td><td>dm_tmax</td><td>Monthly max</td></tr>\n",
    "<tr><td>DayMet Temp Min</td><td>dm_tmin</td><td>Monthly min</td></tr>\n",
    "<tr><td>AdaptWest Mean Annual Temp</td><td>aw_mat</td><td>Yearly avg</td></tr>\n",
    "<tr><td>AdaptWest Mean Temp Warmest Month</td><td>aw_mwmt</td><td>Avg of 1 month</td></tr>\n",
    "<tr><td>AdaptWest Mean Temp Coldest Month</td><td>aw_mcmt</td><td>Avg of 1 month</td></tr>\n",
    "<tr><td>AdaptWest Temp Difference</td><td>aw_td</td><td>Diff of mwmt and mcmt</td></tr>\n",
    "<tr><td>Landfire Vegetation Type</td><td>landfire_fvt</td><td>2020 update</td></tr>\n",
    "<tr><td>Landfire Fuel Model</td><td>landfire_fbfm40</td><td>2020 update</td></tr>\n",
    "<tr><td>Biomass Annuals</td><td>biomass_afg</td><td>Yearly avg</td></tr>\n",
    "<tr><td>Biomass Perennials</td><td>biomass_pfg</td><td>Yearly avg</td></tr>\n",
    "<tr><td>Normalized Difference Vegetation Index</td><td>ndvi</td><td>Weekly avg</td></tr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Running the script\n",
    "The script can be run from the command line with the following command:\n",
    "\n",
    "```python build_mtbs_dataframe.py```\n",
    "\n",
    "##### Some user configuration is required before executing. The following changes need to be made to the script:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DATA_LOC and TMP_LOC paths must be set, as well as the STATE being processed.  The STATE variable is used to filter the MTBS data to only include fires in the state of interest.  The STATE variable must be set to the two letter abbreviation of the state.  For example, to process the state of California, the STATE variable should be set to 'CA'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for temporary storage NOTE: this directory will vary by user\n",
    "DATA_LOC = \"/home/jakebova/Fire_Prediction/Oregon/data\"\n",
    "TMP_LOC = pjoin(DATA_LOC, \"temp\")\n",
    "\n",
    "# the state to process\n",
    "STATE = \"MT\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PATHS object holds the filepaths for the data.  It should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of clipped DEM files\n",
    "DEM_DATA_DIR = pjoin(TMP_LOC, \"dem_data\")\n",
    "\n",
    "# location of feature data files\n",
    "FEATURE_DIR = pjoin(DATA_LOC, \"FeatureData\")\n",
    "EDNA_DIR = pjoin(DATA_LOC, \"terrain\")\n",
    "MTBS_DIR = pjoin(DATA_LOC, \"MTBS_Data\")\n",
    "\n",
    "PATHS = {\n",
    "    \"states\": pjoin(FEATURE_DIR, \"state_borders/cb_2018_us_state_5m.shp\"),\n",
    "    \"dem\": pjoin(EDNA_DIR, \"us_orig_dem/orig_dem/hdr.adf\"),\n",
    "    \"dem_slope\": pjoin(EDNA_DIR, \"us_slope/slope/hdr.adf\"),\n",
    "    \"dem_aspect\": pjoin(EDNA_DIR, \"us_aspect/aspect/hdr.adf\"),\n",
    "    \"dem_flow_acc\": pjoin(EDNA_DIR, \"us_flow_acc/flow_acc/hdr.adf\"),\n",
    "    \"gm_pdsi\": pjoin(FEATURE_DIR, \"gridmet/pdsi_weekly.nc\"),\n",
    "    \"gm_srad\": pjoin(FEATURE_DIR, \"gridmet/srad_weekly.nc\"),\n",
    "    \"gm_vpd\": pjoin(FEATURE_DIR, \"gridmet/vpd_1984-2020_weekly.nc\"),\n",
    "    \"aw_mat\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MAT.tif\"),\n",
    "    \"aw_mcmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MCMT.tif\"),\n",
    "    \"aw_mwmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MWMT.tif\"),\n",
    "    \"aw_td\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_TD.tif\"),\n",
    "    \"dm_tmax\": pjoin(FEATURE_DIR, \"daymet/tmax_monthavg/dm_tmax_1986_2020.nc\"),\n",
    "    \"dm_tmin\": pjoin(FEATURE_DIR, \"daymet/tmin_monthavg/dm_tmin_1986_2020.nc\"),\n",
    "    \"biomass_afg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/1986_2020_biomass_afg_{}.nc\".format(STATE.lower())\n",
    "    ),\n",
    "    \"biomass_pfg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/1986_2020_biomass_pfg_{}.nc\".format(STATE.lower())\n",
    "    ),\n",
    "    \"landfire_fvt\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FVT_200_CONUS/Tif/LC20_FVT_200.tif\"\n",
    "    ),\n",
    "    \"landfire_fbfm40\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FBFM40_200_CONUS/Tif/LC20_F40_200.tif\"\n",
    "    ),\n",
    "    \"ndvi\": pjoin(FEATURE_DIR, \"ndvi/1985_2020_ndvi_weekly.nc\"),\n",
    "    \"mtbs_root\": pjoin(MTBS_DIR, \"MTBS_BSmosaics/\"),\n",
    "    \"mtbs_perim\": pjoin(MTBS_DIR, \"mtbs_perimeter_data/mtbs_perims_DD.shp\"),\n",
    "}\n",
    "YEARS = list(range(1986, 2021))\n",
    "GM_KEYS = list(filter(lambda x: x.startswith(\"gm_\"), PATHS))\n",
    "AW_KEYS = list(filter(lambda x: x.startswith(\"aw_\"), PATHS))\n",
    "DM_KEYS = list(filter(lambda x: x.startswith(\"dm_\"), PATHS))\n",
    "BIOMASS_KEYS = list(filter(lambda x: x.startswith(\"biomass_\"), PATHS))\n",
    "LANDFIRE_KEYS = list(filter(lambda x: x.startswith(\"landfire_\"), PATHS))\n",
    "NDVI_KEYS = list(filter(lambda x: x.startswith(\"ndvi\"), PATHS))\n",
    "DEM_KEYS = list(filter(lambda x: x.startswith(\"dem\"), PATHS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to properly build the dataset, an iterative approach to running the script is required.  There are several ```if``` checks in ```__main__``` that need manual changing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When creating a new dataset, set this block to true and others to false and run the script:\n",
    "```\n",
    "    if 0:\n",
    "        # code below for creating a new dataset for a new state / region\n",
    "        df = build_mtbs_df(\n",
    "            YEARS,\n",
    "            year_to_mtbs_file,\n",
    "            year_to_perims,\n",
    "            STATE,\n",
    "            out_path=mtbs_df_path,\n",
    "        )\n",
    "```\n",
    "\n",
    "2. Once that is done, set the following block to true and others to false and run the script:\n",
    "```\n",
    "    if 0:\n",
    "        df = dgpd.read_parquet(mtbs_df_path)\n",
    "        clip_and_save_dem_rasters(DEM_KEYS, PATHS, state_shape, STATE)\n",
    "        df = add_columns_to_df(\n",
    "            df,\n",
    "            DEM_KEYS,\n",
    "            extract_dem_data,\n",
    "            mtbs_df_temp_path,\n",
    "            # Save results in serial to avoid segfaulting. Something about the\n",
    "            # dem computations makes segfaults extremely likely when saving\n",
    "            # The computations require a lot of memory which may be what\n",
    "            # triggers the fault.\n",
    "            parallel=False,\n",
    "        )\n",
    "\n",
    "```\n",
    "3. After the DEM data is added, the hillshade and year data can be extracted. Set the following block to true and others to false and run the script:\n",
    "```\n",
    "    if 0:\n",
    "        with ProgressBar():\n",
    "            df = dgpd.read_parquet(mtbs_df_path)\n",
    "        df = df.assign(hillshade=U8.type(0))\n",
    "        df = df.map_partitions(hillshade_partition, 45, 180, meta=df._meta)\n",
    "        df = df.assign(year=U16.type(0))\n",
    "        df = df.map_partitions(timestamp_to_year_part, meta=df._meta)\n",
    "\n",
    "        print(df.head())\n",
    "\n",
    "        print(\"Repartitioning and saving \")\n",
    "        df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "        with ProgressBar():\n",
    "            # df.to_parquet(mtbs_df_temp_path)\n",
    "            df.to_parquet(mtbs_df_temp_path)\n",
    "```\n",
    "4. After the hillshade and year data is added, we can start to add the feature data. \n",
    "<br><b>NOTE</b>: This must be done one feature at a time (i.e. DM_KEYS, then NDVI_KEYS, etc)\n",
    "<br>Set the following block to true and others to false and run the script:\n",
    "```\n",
    "    if 0:\n",
    "        # code below used to add new features to the dataset\n",
    "        with ProgressBar():\n",
    "            df = dgpd.read_parquet(mtbs_df_path)\n",
    "        # NOTE: DM KEYS adding to DF on {date}\n",
    "        df = add_columns_to_df(\n",
    "            df, DM KEYS, partition_extract_nc, checkpoint_1_path, parallel=False\n",
    "        )\n",
    "        df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "        print(\"Repartitioning\")\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(mtbs_df_temp_path)\n",
    "```\n",
    "The function passed to ```add_columns_to_df``` depends on the file type of the feature being added.<br>\n",
    "For netCDF, used the ```partition_extract_nc``` function.<br>\n",
    "For TIF data, used the ```extract_tif_data``` function.<br>\n",
    "The mtbs_df_path and mtbs_df_temp_path will have to be swapped when adding new features (i.e. add DM_KEYS, then swap the paths so it reads in the newer dataframe, then add NDVI_KEYS and swap again after, etc)\n",
    "\n",
    "5. When adding netCDF data, NDVI and daymet data requires special attention in the ```netcdf_to_raster``` function.<br>\n",
    "Note the code below from the function:\n",
    "```\n",
    "    # BELOW FOR NDVI ONLY!!!\n",
    "    # nc_ds = nc_ds.drop_vars(\n",
    "    #     [\"latitude_bnds\", \"longitude_bnds\", \"time_bnds\"]\n",
    "    # ).rio.write_crs(\"EPSG:4326\")\n",
    "    # ABOVE FOR NDVI ONLY!!!\n",
    "    # nc_ds = nc_ds.rio.write_crs(\"EPSG:5071\")  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rio.write_crs(\n",
    "    #     nc_ds.coords[\"lambert_conformal_conic\"].spatial_ref\n",
    "    # )  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rename({\"lambert_conformal_conic\": \"crs\"})  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.drop_vars([\"lat\", \"lon\"])  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rename_vars({\"x\": \"lon\", \"y\": \"lat\"})  # FOR DAYMET ONLY!!\n",
    "```\n",
    "\n",
    "If adding other data, the lines stay commented out<br>\n",
    "If adding DM data, uncomment the indicated lines<br>\n",
    "If adding NDVI data, uncomment the indicated lines<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once all the features are added to the dataset, the final product is a Dask GeoDataFrame in parquet format.  The output of df.head() should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "   mtbs                          geometry state    ig_date   gm_pdsi  gm_srad  gm_vpd  aw_mat  aw_mcmt  aw_mwmt      aw_td  ...  dem_aspect  dem_flow_acc  landfire_fvt  landfire_fbfm40  biomass_afg  biomass_pfg    ndvi   dm_tmax    dm_tmin  hillshade  year\n",
    "0     2  POINT (-1830990.000 2456610.000)    OR 1986-03-20  2.779999    164.5    0.41     8.7     -1.9     20.9  22.799999  ...  305.295502           0.0        2967.0            102.0        165.0        891.0  0.0782 -3.049677 -16.621613        179  1986\n",
    "1     2  POINT (-1830960.000 2456610.000)    OR 1986-03-20  2.779999    164.5    0.41     8.7     -1.9     20.9  22.799999  ...  333.025360           0.0        2967.0            102.0        411.0        638.0  0.0782 -3.049677 -16.621613        179  1986\n",
    "2     2  POINT (-1830990.000 2456580.000)    OR 1986-03-20  2.779999    164.5    0.41     8.7     -1.9     20.9  22.799999  ...  253.358459           0.0        2967.0            102.0        388.0        508.0  0.0782 -3.049677 -16.621613        181  1986\n",
    "3     2  POINT (-1830960.000 2456580.000)    OR 1986-03-20  2.779999    164.5    0.41     8.7     -1.9     20.9  22.799999  ...  254.867813           0.0        2080.0            122.0        411.0        638.0  0.0782 -3.049677 -16.621613        181  1986\n",
    "4     2  POINT (-1830930.000 2456580.000)    OR 1986-03-20  2.779999    164.5    0.41     8.7     -1.9     20.9  22.799999  ...  339.071350           0.0        2123.0            122.0        379.0        492.0  0.0782 -3.049677 -16.621613        180  1986"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the dataset is constructed, the model can be trained.  The model is trained using the ```rf_class_mtbs.py``` script.  The script can be run from the command line with the following command:\n",
    "\n",
    "```python rf_class_mtbs.py```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use a random forest classifier with mostly default parameters to predict the class of each cell in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are several options for training the model.  The following changes can be made to the script:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "```\n",
    "register_ray()\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "p = 0.2\n",
    "SEED = 42\n",
    "TREES = 50\n",
    "filename = \"rf_mtbs_oregon_50t20d.sav\"\n",
    "TRAIN = True\n",
    "TEST = False\n",
    "\n",
    "STATE = \"OR\"\n",
    "DATA_LOC = \"/home/jakebova/Fire_Prediction/Oregon/data\"\n",
    "TMP_LOC = pjoin(DATA_LOC, \"temp\")\n",
    "\n",
    "DF_LOC = pjoin(TMP_LOC, f\"{STATE}_mtbs_df_with_ids\")\n",
    "```\n",
    "\n",
    "The following changes can be made:\n",
    "- ```p``` is the percentage of the dataset to use for training.  The default is 0.2, which is 20% of the dataset.  This can be changed to any value between 0 and 1. Note that this requires a large amount of memory.  If the script crashes, try reducing the value of ```p```.\n",
    "- ```SEED``` is the random seed used for the model.  The default is 42, but this can be changed to any integer.\n",
    "- ```TREES``` is the number of trees to use in the model.  The default is 50, but this can be changed to any integer.  We have found best success in the 100 to 150 range. Note that this requires a large amount of memory.  If the script crashes, try reducing the value of ```TREES```.\n",
    "- ```filename``` is the name of the file to save the model to.  The default is ```rf_mtbs_oregon_50t20d.sav```, but this can be changed to any string.\n",
    "- ```TRAIN``` is a boolean value that determines whether or not to train the model.  The default is ```True```, but this can be changed to ```False``` to load a previously trained model.\n",
    "- ```TEST``` is a boolean value that determines whether or not to test the model.  The default is ```False```, but this can be changed to ```True``` to test the model.\n",
    "\n",
    "\n",
    "The verbosity of the scikit-learn random forest classifier can be changed by changing the value of verbose (typically 0, 1, or 2) in the following line:\n",
    "```\n",
    "forest = RandomForestClassifier(n_estimators=TREES, verbose=1, random_state=SEED)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once complete (anywhere from ~30 minutes to several hours depending on configuration), the model will be saved to the ```filename``` location. The model can then be used to predict the class of any dataset with the same features as the training dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The output of model training script should look something like this:\n",
    "\n",
    "```\n",
    "Sampling 40.0% of data\n",
    "Cleaning data\n",
    "Splitting data into train and test sets\n",
    "Instantiating model with 25 trees\n",
    "Training model\n",
    "****************************************\n",
    "Accuracy score: 0.8380\n",
    "****************************************\n",
    "****************************************\n",
    "Confusion matrix:\n",
    "[[1583439  399576   28033    5013]\n",
    " [ 279435 4527912  250977   23102]\n",
    " [  32641  352893 1482150  121551]\n",
    " [   5747   25944  130280  968496]]\n",
    "****************************************\n",
    "****************************************\n",
    "Classification Report\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           1       0.83      0.79      0.81   2016061\n",
    "           2       0.85      0.89      0.87   5081426\n",
    "           3       0.78      0.75      0.76   1989235\n",
    "           4       0.87      0.86      0.86   1130467\n",
    "\n",
    "    accuracy                           0.84  10217189\n",
    "   macro avg       0.83      0.82      0.83  10217189\n",
    "weighted avg       0.84      0.84      0.84  10217189\n",
    "\n",
    "****************************************\n",
    "****************************************\n",
    "Area under ROC Curve:\n",
    "0.9616861528386682\n",
    "****************************************\n",
    "****************************************\n",
    "Feature Importances (Impurity based):\n",
    "dem                0.097426\n",
    "biomass_pfg        0.094367\n",
    "dem_aspect         0.087795\n",
    "dem_slope          0.084736\n",
    "biomass_afg        0.079713\n",
    "dm_tmax            0.054383\n",
    "dm_tmin            0.053868\n",
    "ndvi               0.050503\n",
    "dem_flow_acc       0.050460\n",
    "gm_srad            0.049898\n",
    "aw_mwmt            0.044709\n",
    "gm_pdsi            0.039106\n",
    "aw_td              0.038576\n",
    "landfire_fbfm40    0.038562\n",
    "landfire_fvt       0.036355\n",
    "gm_vpd             0.035572\n",
    "aw_mat             0.032097\n",
    "aw_mcmt            0.031874\n",
    "dtype: float64\n",
    "END RUN .. dumping files\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rstools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00a9f65607a6ad9a572a8754f994c98022b3f03e153a160dac8529d25bd50f52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
