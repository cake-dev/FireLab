{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy.fft import dst\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from raster_tools import Raster, Vector, open_vectors, clipping, zonal\n",
    "from raster_tools.dtypes import F32, U8\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for temporary storage\n",
    "TMP_LOC = \"/home/jake/FireLab/Project/data/temp/\"\n",
    "DATA_LOC = \"/home/jake/FireLab/Project/data/\"\n",
    "\n",
    "STATE = \"OR\"\n",
    "\n",
    "# Location of clipped DEM files\n",
    "DEM_DATA_DIR = pjoin(TMP_LOC, \"dem_data\")\n",
    "\n",
    "# location of feature data files\n",
    "FEATURE_DIR = pjoin(DATA_LOC, \"FeatureData\")\n",
    "EDNA_DIR = pjoin(DATA_LOC, \"terrain\")\n",
    "MTBS_DIR = pjoin(DATA_LOC, \"MTBS_Data\")\n",
    "\n",
    "mtbs_df_path = pjoin(TMP_LOC, f\"{STATE}_mtbs.parquet\")\n",
    "mtbs_df_temp_path = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp.parquet\")\n",
    "checkpoint_1_path = pjoin(TMP_LOC, \"check1\")\n",
    "checkpoint_2_path = pjoin(TMP_LOC, \"check2\")\n",
    "\n",
    "PATHS = {\n",
    "    \"states\": pjoin(EDNA_DIR, \"state_borders/cb_2018_us_state_5m.shp\"),\n",
    "    \"dem\": pjoin(EDNA_DIR, \"us_orig_dem/us_orig_dem/orig_dem/hdr.adf\"),\n",
    "    \"dem_slope\": pjoin(EDNA_DIR, \"us_slope/us_slope/slope/hdr.adf\"),\n",
    "    \"dem_aspect\": pjoin(EDNA_DIR, \"us_aspect/aspect/hdr.adf\"),\n",
    "    \"dem_flow_acc\": pjoin(EDNA_DIR, \"us_flow_acc/us_flow_acc/flow_acc/hdr.adf\"),\n",
    "    \"gm_srad\": pjoin(FEATURE_DIR, \"gridmet/srad_1986_2020_weekly.nc\"),\n",
    "    \"gm_vpd\": pjoin(FEATURE_DIR, \"gridmet/vpd_1986_2020_weekly.nc\"),\n",
    "    \"aw_mat\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MAT.tif\"),\n",
    "    \"aw_mcmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MCMT.tif\"),\n",
    "    \"aw_mwmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MWMT.tif\"),\n",
    "    \"aw_td\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_TD.tif\"),\n",
    "    \"dm_tmax\": pjoin(FEATURE_DIR, \"daymet/tmax_1986_2020.nc\"),\n",
    "    \"dm_tmin\": pjoin(FEATURE_DIR, \"daymet/tmin_1986_2020.nc\"),\n",
    "    \"biomass_afg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_afg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"biomass_pfg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_pfg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"landfire_fvt\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FVT_200_CONUS/Tif/LC20_FVT_200.tif\"\n",
    "    ),\n",
    "    \"landfire_fbfm40\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FBFM40_200_CONUS/Tif/LC20_F40_200.tif\"\n",
    "    ),\n",
    "    \"ndvi\": pjoin(FEATURE_DIR, \"ndvi/access/weekly/ndvi_1986_2020_weekavg.nc\"),\n",
    "    \"mtbs_root\": pjoin(MTBS_DIR, \"MTBS_BSmosaics/\"),\n",
    "    \"mtbs_perim\": pjoin(MTBS_DIR, \"mtbs_perimeter_data/mtbs_perims_DD.shp\"),\n",
    "}\n",
    "YEARS = list(range(1986, 2021))\n",
    "GM_KEYS = list(filter(lambda x: x.startswith(\"gm_\"), PATHS))\n",
    "AW_KEYS = list(filter(lambda x: x.startswith(\"aw_\"), PATHS))\n",
    "DM_KEYS = list(filter(lambda x: x.startswith(\"dm_\"), PATHS))\n",
    "BIOMASS_KEYS = list(filter(lambda x: x.startswith(\"biomass_\"), PATHS))\n",
    "LANDFIRE_KEYS = list(filter(lambda x: x.startswith(\"landfire_\"), PATHS))\n",
    "NDVI_KEYS = list(filter(lambda x: x.startswith(\"ndvi\"), PATHS))\n",
    "DEM_KEYS = list(filter(lambda x: x.startswith(\"dem\"), PATHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nc_var_name(ds):\n",
    "    # Find the data variable in a nc xarray.Dataset\n",
    "    # var_name = list(set(ds.keys()) - set([\"crs\", \"day_bnds\"]))[0]\n",
    "    var_name = list(set(ds.keys()) - set([\"crs\", \"bnds\"]))[1]\n",
    "    return var_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_to_raster(path, date):\n",
    "    # This produces a Dataset. We need to grab the DataArray inside that\n",
    "    # contains the data of interest.\n",
    "    nc_ds = xr.open_dataset(path, chunks={\"day\": 1})#, decode_times=False)\n",
    "    # nc_ds = nc_ds.rio.write_crs(\"EPSG:5071\")  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rio.write_crs(\n",
    "    #     nc_ds.coords[\"lambert_conformal_conic\"].spatial_ref\n",
    "    # )  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rename({\"lambert_conformal_conic\": \"crs\"})  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.drop_vars([\"lat\", \"lon\"])  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rename_vars({\"x\": \"lon\", \"y\": \"lat\"})  # FOR DAYMET ONLY!!\n",
    "    # comment lines below for normal operation\n",
    "    # ds_crs = CRS.from_epsg(5071)\n",
    "    # nc_ds.rio.write_crs(ds_crs)\n",
    "    nc_ds = nc_ds.rio.write_crs(nc_ds.crs.spatial_ref)\n",
    "    # print nc_ds dimensions\n",
    "    print(f\"{nc_ds.dims = }\")\n",
    "    # Find variable name\n",
    "    var_name = get_nc_var_name(nc_ds)\n",
    "    print(f\"var_name: {var_name}\")\n",
    "    # Extract\n",
    "    var_da = nc_ds[var_name]\n",
    "    print(f\"{var_da = }\")\n",
    "    var_da = var_da.sel(time=date, method=\"nearest\")\n",
    "    xrs = xr.DataArray(\n",
    "        var_da.data, dims=(\"y\", \"x\"), coords=(var_da.lat.data, var_da.lon.data)\n",
    "    ).expand_dims(\"band\")\n",
    "    xrs[\"band\"] = [1]\n",
    "    # Set CRS in raster compliant format\n",
    "    xrs = xrs.rio.write_crs(nc_ds.crs.spatial_ref)\n",
    "    return Raster(xrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nc_data(df, gm_name):\n",
    "    assert df.ig_date.unique().size == 1\n",
    "    # print(f\"{gm_name}: {df.columns = }, {len(df) = }\")\n",
    "    date = df.ig_date.values[0]\n",
    "    print(f\"{gm_name}: starting {date}\")\n",
    "    rs = netcdf_to_raster(PATHS[gm_name], date)\n",
    "    bounds = gpd.GeoSeries(df.geometry).to_crs(rs.crs).total_bounds\n",
    "    rs = clipping.clip_box(rs, bounds)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .rename(columns={\"extracted\": gm_name})\n",
    "        .compute()\n",
    "    )\n",
    "    df[gm_name].values[:] = rdf[gm_name].values\n",
    "    # print(f\"{gm_name}: finished {date}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_extract_nc(df, key):\n",
    "    # This func wraps extract_nc_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_nc_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_nc_data(gdf, key))\n",
    "    return pd.concat(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns_to_df(\n",
    "    df,\n",
    "    columns,\n",
    "    part_func,\n",
    "    out_path,\n",
    "    col_type=F32,\n",
    "    col_default=np.nan,\n",
    "    part_func_args=(),\n",
    "    tmp_loc=TMP_LOC,\n",
    "    parallel=True,\n",
    "):\n",
    "    print(f\"Adding columns: {columns}\")\n",
    "    # Add columns\n",
    "    expanded_df = df.assign(**{c: col_type.type(col_default) for c in columns})\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        # Save to disk before applying partition function. to_parquet() has a\n",
    "        # chance of segfaulting and that chance goes WAY up after adding\n",
    "        # columns and then mapping a function to partitions. Saving to disk\n",
    "        # before mapping keeps the odds low.\n",
    "        path = pjoin(working_dir, \"expanded\")\n",
    "        expanded_df.to_parquet(path)\n",
    "\n",
    "        expanded_df = dgpd.read_parquet(path)\n",
    "        meta = expanded_df._meta.copy()\n",
    "        for c in columns:\n",
    "            expanded_df = expanded_df.map_partitions(\n",
    "                part_func, c, *part_func_args, meta=meta\n",
    "            )\n",
    "\n",
    "        if parallel:\n",
    "            with ProgressBar():\n",
    "                expanded_df.to_parquet(out_path)\n",
    "        else:\n",
    "            # Save parts in serial and then assemble into single dataframe\n",
    "            with tempfile.TemporaryDirectory(dir=tmp_loc) as part_dir:\n",
    "                dfs = []\n",
    "                for i, part in enumerate(expanded_df.partitions):\n",
    "                    # Save part i\n",
    "                    part_path = pjoin(part_dir, f\"part{i:04}\")\n",
    "                    with ProgressBar():\n",
    "                        part.compute().to_parquet(part_path)\n",
    "                    # Save paths for opening with dask_geopandas later. Avoid\n",
    "                    # opening more dataframes in this loop as doing so will\n",
    "                    # likely cause a segfault. I have no idea why.\n",
    "                    dfs.append(part_path)\n",
    "                dfs = [dgpd.read_parquet(p) for p in dfs]\n",
    "                # Assemble and save to final output location\n",
    "                expanded_df = dd.concat(dfs)\n",
    "                with ProgressBar():\n",
    "                    expanded_df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code below used to add new features to the dataset\n",
    "with ProgressBar():\n",
    "    df = dgpd.read_parquet(checkpoint_2_path)\n",
    "df = add_columns_to_df(\n",
    "    df, DM_KEYS, partition_extract_nc, checkpoint_1_path, parallel=False\n",
    ")\n",
    "df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "print(\"Repartitioning\")\n",
    "with ProgressBar():\n",
    "    df.to_parquet(mtbs_df_temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    df = dgpd.read_parquet(checkpoint_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_ds = xr.open_dataset(PATHS[\"aw_mat\"], chunks={\"day\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_ds.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for comparing and displaying data / coords / info from a collection of xarray datasets\n",
    "def compare_xr_datasets(datasets, attrs=False, coords=False, info=False):\n",
    "    for ds in datasets:\n",
    "        print(f\"{ds = }\")\n",
    "        if attrs:\n",
    "            print(f\"{ds.attrs = }\")\n",
    "        if coords:\n",
    "            print(f\"{ds.coords = }\")\n",
    "        if info:\n",
    "            print(f\"{ds.info = }\")\n",
    "        print(\"-----------------\\n\\n\")\n",
    "\n",
    "nc_datasets = [xr.open_dataset(PATHS['dm_tmax'], chunks={\"day\": 1}), xr.open_dataset(PATHS['aw_mat'], chunks={\"day\": 1})]\n",
    "compare_xr_datasets(nc_datasets, attrs=True, coords=True, info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_ds = xr.open_dataset(PATHS['dm_tmax'], chunks={\"day\": 1})#, decode_times=False)\n",
    "nc_ds = nc_ds.rio.write_crs(\"EPSG:5071\")  # FOR DAYMET ONLY!!\n",
    "nc_ds = nc_ds.rio.write_crs(\n",
    "    nc_ds.coords[\"lambert_conformal_conic\"].spatial_ref\n",
    ")  # FOR DAYMET ONLY!!\n",
    "nc_ds = nc_ds.rename({\"lambert_conformal_conic\": \"crs\"})  # FOR DAYMET ONLY!!\n",
    "nc_ds2 = nc_ds.drop_vars([\"lat\", \"lon\"])  # FOR DAYMET ONLY!!\n",
    "nc_ds2 = nc_ds2.rename_vars({\"x\": \"lon\", \"y\": \"lat\"})  # FOR DAYMET ONLY!!\n",
    "nc_ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ig_date.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually map/draw the fire from df with ig_date = 1986-03-20\n",
    "import matplotlib.pyplot as plt\n",
    "# plot with oregon map as background\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "states = gpd.read_file(PATHS['states'])\n",
    "states = states.to_crs(df.crs)\n",
    "# limit states to just oregon\n",
    "states = states[states.STUSPS == 'OR']\n",
    "states.plot(ax=ax, color='white', edgecolor='black')\n",
    "df[df.ig_date == '2014-07-14'].plot(ax=ax, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = df.ig_date.values[0]\n",
    "var_name = get_nc_var_name(nc_ds2)\n",
    "# print(f\"var_name: {var_name}\")\n",
    "# Extract\n",
    "var_da = nc_ds2[var_name]\n",
    "var_da = var_da.sel(time=date, method=\"nearest\")\n",
    "print(f\"{var_da = }\")\n",
    "xrs = xr.DataArray(\n",
    "    var_da.data, dims=(\"y\", \"x\"), coords=(var_da.lat.data, var_da.lon.data)\n",
    ").expand_dims(\"band\")\n",
    "xrs[\"band\"] = [1]\n",
    "# Set CRS in raster compliant format\n",
    "xrs = xrs.rio.write_crs(nc_ds2.crs.spatial_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dgpd.read_parquet(mtbs_df_temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mtbs</th>\n",
       "      <th>geometry</th>\n",
       "      <th>state</th>\n",
       "      <th>ig_date</th>\n",
       "      <th>dem</th>\n",
       "      <th>dem_slope</th>\n",
       "      <th>dem_aspect</th>\n",
       "      <th>dem_flow_acc</th>\n",
       "      <th>dm_tmax</th>\n",
       "      <th>dm_tmin</th>\n",
       "      <th>biomass_afg</th>\n",
       "      <th>biomass_pfg</th>\n",
       "      <th>gm_srad</th>\n",
       "      <th>gm_vpd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-1830990.000 2456610.000)</td>\n",
       "      <td>OR</td>\n",
       "      <td>1986-03-20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.559452</td>\n",
       "      <td>305.295502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.049677</td>\n",
       "      <td>-16.621613</td>\n",
       "      <td>165.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>174.600006</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-1830960.000 2456610.000)</td>\n",
       "      <td>OR</td>\n",
       "      <td>1986-03-20</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>2.485488</td>\n",
       "      <td>128.388641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.049677</td>\n",
       "      <td>-16.621613</td>\n",
       "      <td>411.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>174.600006</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-1830990.000 2456580.000)</td>\n",
       "      <td>OR</td>\n",
       "      <td>1986-03-20</td>\n",
       "      <td>1409.0</td>\n",
       "      <td>10.548834</td>\n",
       "      <td>277.054626</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.049677</td>\n",
       "      <td>-16.621613</td>\n",
       "      <td>388.0</td>\n",
       "      <td>508.0</td>\n",
       "      <td>174.600006</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-1830960.000 2456580.000)</td>\n",
       "      <td>OR</td>\n",
       "      <td>1986-03-20</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>7.670347</td>\n",
       "      <td>30.625286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.049677</td>\n",
       "      <td>-16.621613</td>\n",
       "      <td>411.0</td>\n",
       "      <td>638.0</td>\n",
       "      <td>174.600006</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>POINT (-1830930.000 2456580.000)</td>\n",
       "      <td>OR</td>\n",
       "      <td>1986-03-20</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>1.268448</td>\n",
       "      <td>124.799530</td>\n",
       "      <td>43.0</td>\n",
       "      <td>-3.049677</td>\n",
       "      <td>-16.621613</td>\n",
       "      <td>379.0</td>\n",
       "      <td>492.0</td>\n",
       "      <td>174.600006</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mtbs                          geometry state    ig_date     dem  dem_slope  \\\n",
       "0     2  POINT (-1830990.000 2456610.000)    OR 1986-03-20  1261.0   0.559452   \n",
       "1     2  POINT (-1830960.000 2456610.000)    OR 1986-03-20  1277.0   2.485488   \n",
       "2     2  POINT (-1830990.000 2456580.000)    OR 1986-03-20  1409.0  10.548834   \n",
       "3     2  POINT (-1830960.000 2456580.000)    OR 1986-03-20  1545.0   7.670347   \n",
       "4     2  POINT (-1830930.000 2456580.000)    OR 1986-03-20  1407.0   1.268448   \n",
       "\n",
       "   dem_aspect  dem_flow_acc   dm_tmax    dm_tmin  biomass_afg  biomass_pfg  \\\n",
       "0  305.295502           0.0 -3.049677 -16.621613        165.0        891.0   \n",
       "1  128.388641           0.0 -3.049677 -16.621613        411.0        638.0   \n",
       "2  277.054626           6.0 -3.049677 -16.621613        388.0        508.0   \n",
       "3   30.625286           0.0 -3.049677 -16.621613        411.0        638.0   \n",
       "4  124.799530          43.0 -3.049677 -16.621613        379.0        492.0   \n",
       "\n",
       "      gm_srad  gm_vpd  \n",
       "0  174.600006    0.54  \n",
       "1  174.600006    0.54  \n",
       "2  174.600006    0.54  \n",
       "3  174.600006    0.54  \n",
       "4  174.600006    0.54  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_ds = xr.open_dataset(PATHS[\"ndvi\"], chunks={\"time\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "CRSError",
     "evalue": "Invalid projection: EPSG:<xarray.DataArray 'crs' ()>\n[1 values with dtype=int16]\nAttributes:\n    grid_mapping_name:            latitude_longitude\n    epsg_code:                    EPSG:4326\n    longitude_of_prime_meridian:  0.0\n    semi_major_axis:              6378137.0\n    inverse_flattening:           298.257223563: (Internal Proj Error: proj_create: unrecognized format / unknown name)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCRSError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ndvi_ds\u001b[39m.\u001b[39;49mrio\u001b[39m.\u001b[39;49mwrite_crs(ndvi_ds\u001b[39m.\u001b[39;49mcrs)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/rioxarray/rioxarray.py:474\u001b[0m, in \u001b[0;36mXRasterBase.write_crs\u001b[0;34m(self, input_crs, grid_mapping_name, inplace)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[39mWrite the CRS to the dataset in a CF compliant manner.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[39m>>> raster = raster.rio.write_crs(\"epsg:4326\")\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m input_crs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     data_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_crs(input_crs, inplace\u001b[39m=\u001b[39;49minplace)\n\u001b[1;32m    475\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     data_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obj(inplace\u001b[39m=\u001b[39minplace)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/rioxarray/rioxarray.py:363\u001b[0m, in \u001b[0;36mXRasterBase.set_crs\u001b[0;34m(self, input_crs, inplace)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_crs\u001b[39m(\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39m, input_crs: Any, inplace: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    346\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[xarray\u001b[39m.\u001b[39mDataset, xarray\u001b[39m.\u001b[39mDataArray]:\n\u001b[1;32m    347\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    Set the CRS value for the Dataset/DataArray without modifying\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m    the dataset/data array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[39m        Dataset with crs attribute.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     crs \u001b[39m=\u001b[39m crs_from_user_input(input_crs)\n\u001b[1;32m    364\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obj(inplace\u001b[39m=\u001b[39minplace)\n\u001b[1;32m    365\u001b[0m     obj\u001b[39m.\u001b[39mrio\u001b[39m.\u001b[39m_crs \u001b[39m=\u001b[39m crs\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/rioxarray/crs.py:42\u001b[0m, in \u001b[0;36mcrs_from_user_input\u001b[0;34m(crs_input)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# use pyproj for edge cases\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m crs \u001b[39m=\u001b[39m CRS\u001b[39m.\u001b[39;49mfrom_user_input(crs_input)\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(rasterio\u001b[39m.\u001b[39m__gdal_version__) \u001b[39m>\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m3.0.0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m rasterio\u001b[39m.\u001b[39mcrs\u001b[39m.\u001b[39mCRS\u001b[39m.\u001b[39mfrom_wkt(crs\u001b[39m.\u001b[39mto_wkt())\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pyproj/crs/crs.py:501\u001b[0m, in \u001b[0;36mCRS.from_user_input\u001b[0;34m(cls, value, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mcls\u001b[39m):\n\u001b[1;32m    500\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m--> 501\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(value, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pyproj/crs/crs.py:348\u001b[0m, in \u001b[0;36mCRS.__init__\u001b[0;34m(self, projparams, **kwargs)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local\u001b[39m.\u001b[39mcrs \u001b[39m=\u001b[39m projparams\n\u001b[1;32m    347\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local\u001b[39m.\u001b[39mcrs \u001b[39m=\u001b[39m _CRS(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrs)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pyproj/_crs.pyx:2378\u001b[0m, in \u001b[0;36mpyproj._crs._CRS.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCRSError\u001b[0m: Invalid projection: EPSG:<xarray.DataArray 'crs' ()>\n[1 values with dtype=int16]\nAttributes:\n    grid_mapping_name:            latitude_longitude\n    epsg_code:                    EPSG:4326\n    longitude_of_prime_meridian:  0.0\n    semi_major_axis:              6378137.0\n    inverse_flattening:           298.257223563: (Internal Proj Error: proj_create: unrecognized format / unknown name)"
     ]
    }
   ],
   "source": [
    "ndvi_ds.rio.write_crs(ndvi_ds.crs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
