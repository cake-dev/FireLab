{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy.fft import dst\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from raster_tools import Raster, Vector, open_vectors, clipping, zonal\n",
    "from raster_tools.dtypes import F32, U8, U16\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change pandas max col display\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for temporary storage\n",
    "TMP_LOC = \"/home/jake/FireLab/Project/data/temp/\"\n",
    "DATA_LOC = \"/home/jake/FireLab/Project/data/\"\n",
    "\n",
    "STATE = \"OR\"\n",
    "\n",
    "# Location of clipped DEM files\n",
    "DEM_DATA_DIR = pjoin(TMP_LOC, \"dem_data\")\n",
    "\n",
    "# location of feature data files\n",
    "FEATURE_DIR = pjoin(DATA_LOC, \"FeatureData\")\n",
    "EDNA_DIR = pjoin(DATA_LOC, \"terrain\")\n",
    "MTBS_DIR = pjoin(DATA_LOC, \"MTBS_Data\")\n",
    "\n",
    "mtbs_df_path = pjoin(TMP_LOC, f\"{STATE}_mtbs.parquet/\")\n",
    "mtbs_df_parquet_path = pjoin(TMP_LOC, f\"{STATE}_mtbs_new.parquet\")\n",
    "mtbs_df_temp_path = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp.parquet/\")\n",
    "checkpoint_1_path = pjoin(TMP_LOC, \"check1\")\n",
    "checkpoint_2_path = pjoin(TMP_LOC, \"check2\")\n",
    "\n",
    "PATHS = {\n",
    "    \"states\": pjoin(EDNA_DIR, \"state_borders/cb_2018_us_state_5m.shp\"),\n",
    "    \"dem\": pjoin(EDNA_DIR, \"us_orig_dem/us_orig_dem/orig_dem/hdr.adf\"),\n",
    "    \"dem_slope\": pjoin(EDNA_DIR, \"us_slope/us_slope/slope/hdr.adf\"),\n",
    "    \"dem_aspect\": pjoin(EDNA_DIR, \"us_aspect/aspect/hdr.adf\"),\n",
    "    \"dem_flow_acc\": pjoin(EDNA_DIR, \"us_flow_acc/us_flow_acc/flow_acc/hdr.adf\"),\n",
    "    \"gm_srad\": pjoin(FEATURE_DIR, \"gridmet/srad_1986_2020_weekly.nc\"),\n",
    "    \"gm_vpd\": pjoin(FEATURE_DIR, \"gridmet/vpd_1986_2020_weekly.nc\"),\n",
    "    \"aw_mat\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MAT.tif\"),\n",
    "    \"aw_mcmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MCMT.tif\"),\n",
    "    \"aw_mwmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MWMT.tif\"),\n",
    "    \"aw_td\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_TD.tif\"),\n",
    "    \"dm_tmax\": pjoin(FEATURE_DIR, \"daymet/tmax_1986_2020.nc\"),\n",
    "    \"dm_tmin\": pjoin(FEATURE_DIR, \"daymet/tmin_1986_2020.nc\"),\n",
    "    \"biomass_afg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_afg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"biomass_pfg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_pfg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"landfire_fvt\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FVT_200_CONUS/Tif/LC20_FVT_200.tif\"\n",
    "    ),\n",
    "    \"landfire_fbfm40\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FBFM40_200_CONUS/Tif/LC20_F40_200.tif\"\n",
    "    ),\n",
    "    \"ndvi\": pjoin(FEATURE_DIR, \"ndvi/access/weekly/ndvi_1986_2020_weekavg.nc\"),\n",
    "    \"mtbs_root\": pjoin(MTBS_DIR, \"MTBS_BSmosaics/\"),\n",
    "    \"mtbs_perim\": pjoin(MTBS_DIR, \"mtbs_perimeter_data/mtbs_perims_DD.shp\"),\n",
    "}\n",
    "YEARS = list(range(2016, 2021))\n",
    "GM_KEYS = list(filter(lambda x: x.startswith(\"gm_\"), PATHS))\n",
    "AW_KEYS = list(filter(lambda x: x.startswith(\"aw_\"), PATHS))\n",
    "DM_KEYS = list(filter(lambda x: x.startswith(\"dm_\"), PATHS))\n",
    "BIOMASS_KEYS = list(filter(lambda x: x.startswith(\"biomass_\"), PATHS))\n",
    "LANDFIRE_KEYS = list(filter(lambda x: x.startswith(\"landfire_\"), PATHS))\n",
    "NDVI_KEYS = list(filter(lambda x: x.startswith(\"ndvi\"), PATHS))\n",
    "DEM_KEYS = list(filter(lambda x: x.startswith(\"dem\"), PATHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020 = dgpd.read_parquet(checkpoint_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020_computed_dem_only = mtbs_df_2016_2020.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020_computed_dem_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MTBS DATASET ADD UID AND EVENT ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dask_df = dgpd.read_parquet(mtbs_df_temp_path)\n",
    "mtbs_dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset = mtbs_dask_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mtbs perimeters\n",
    "print(\"Loading MTBS perimeters\")\n",
    "mtbs_perim = gpd.read_file(PATHS[\"mtbs_perim\"])\n",
    "mtbs_perim['Ig_Date'] = pd.to_datetime(mtbs_perim['Ig_Date'])\n",
    "mtbs_perim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the columns we need (Event_ID where startswith OR, Ig_Date, and geometry)\n",
    "mtbs_perim = mtbs_perim[[\"Event_ID\", \"Ig_Date\", \"geometry\"]]\n",
    "mtbs_perim = mtbs_perim[mtbs_perim.Event_ID.str.startswith(\"OR\")]\n",
    "# drop rows where Ig_Date before 1986 or after 2020\n",
    "mtbs_perim = mtbs_perim[mtbs_perim.Ig_Date.dt.year.between(2018, 2020)]\n",
    "mtbs_perim.reset_index(drop=True, inplace=True)\n",
    "len(mtbs_perim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_perim.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dask_df.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the geometry in mtbs_dataset to lat lon\n",
    "# mtbs_dask_df = mtbs_dask_df.to_crs(epsg=4326)\n",
    "mtbs_perim = mtbs_perim.to_crs(mtbs_df_2016_2020.crs)\n",
    "# mtbs_dataset = mtbs_dataset.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_perim.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_perim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join(partition, mtbs_perim):\n",
    "    # Convert the Dask partition to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(partition, geometry='geometry', crs=mtbs_perim.crs)\n",
    "\n",
    "    # Perform the spatial join\n",
    "    joined = gpd.sjoin(gdf, mtbs_perim, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    # Filter by date if needed\n",
    "    joined = joined[joined['ig_date'] == joined['Ig_Date']]\n",
    "    # print(joined)\n",
    "\n",
    "    return joined\n",
    "\n",
    "result = mtbs_df_2016_2020.map_partitions(spatial_join, mtbs_perim, align_dataframes=False)\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    final_result = result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare final_results and mtbs_dataset to see what is missing from final_results after the spatial join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.drop(columns=['index_right', 'Ig_Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "final_result.dropna(inplace=True)\n",
    "# adds unique id to each pixel\n",
    "final_result.reset_index(inplace=True)\n",
    "final_result['index'] = final_result.index\n",
    "# rename index to unique_id\n",
    "final_result.rename(columns={'index': 'unique_id'}, inplace=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a fire topo map\n",
    "\n",
    "eventID = 'OR4380511789220200816'\n",
    "\n",
    "fire = final_result[final_result.Event_ID == eventID]\n",
    "\n",
    "col = 'dem'\n",
    "\n",
    "fire.plot(column=col, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write final_result to dask so we can write to parquet\n",
    "# final_result = dd.from_pandas(final_result, npartitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    final_result.to_parquet(mtbs_df_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 128.62 s\n"
     ]
    }
   ],
   "source": [
    "# read and check\n",
    "mtbs_dataset_final = dgpd.read_parquet(mtbs_df_parquet_path)\n",
    "with ProgressBar():\n",
    "    mtbs_final_computed = mtbs_dataset_final.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a fire topo map\n",
    "\n",
    "eventID = 'OR4380511789220200816'\n",
    "\n",
    "fire = mtbs_final_computed[mtbs_final_computed.Event_ID == eventID]\n",
    "\n",
    "col = 'dem'\n",
    "\n",
    "fire.plot(column=col, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_2020_08_16 = mtbs_dataset[(mtbs_dataset.year == 2020) & (mtbs_dataset.ig_date.dt.month == 8) & (mtbs_dataset.ig_date.dt.day == 16)]\n",
    "mtbs_dataset_2020_08_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_2020_08_16.plot(column=col, s=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
