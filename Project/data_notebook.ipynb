{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy.fft import dst\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from raster_tools import Raster, Vector, open_vectors, clipping, zonal\n",
    "from raster_tools.dtypes import F32, U8, U16\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change pandas max col display\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for temporary storage\n",
    "TMP_LOC = \"/home/jake/FireLab/Project/data/temp/\"\n",
    "DATA_LOC = \"/home/jake/FireLab/Project/data/\"\n",
    "\n",
    "STATE = \"OR\"\n",
    "\n",
    "# Location of clipped DEM files\n",
    "DEM_DATA_DIR = pjoin(TMP_LOC, \"dem_data\")\n",
    "\n",
    "# location of feature data files\n",
    "FEATURE_DIR = pjoin(DATA_LOC, \"FeatureData\")\n",
    "EDNA_DIR = pjoin(DATA_LOC, \"terrain\")\n",
    "MTBS_DIR = pjoin(DATA_LOC, \"MTBS_Data\")\n",
    "\n",
    "mtbs_df_path = pjoin(TMP_LOC, f\"{STATE}_mtbs.parquet/\")\n",
    "mtbs_df_temp_path = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp.parquet/\")\n",
    "checkpoint_1_path = pjoin(TMP_LOC, \"check1\")\n",
    "checkpoint_2_path = pjoin(TMP_LOC, \"check2\")\n",
    "\n",
    "PATHS = {\n",
    "    \"states\": pjoin(EDNA_DIR, \"state_borders/cb_2018_us_state_5m.shp\"),\n",
    "    \"dem\": pjoin(EDNA_DIR, \"us_orig_dem/us_orig_dem/orig_dem/hdr.adf\"),\n",
    "    \"dem_slope\": pjoin(EDNA_DIR, \"us_slope/us_slope/slope/hdr.adf\"),\n",
    "    \"dem_aspect\": pjoin(EDNA_DIR, \"us_aspect/aspect/hdr.adf\"),\n",
    "    \"dem_flow_acc\": pjoin(EDNA_DIR, \"us_flow_acc/us_flow_acc/flow_acc/hdr.adf\"),\n",
    "    \"gm_srad\": pjoin(FEATURE_DIR, \"gridmet/srad_1986_2020_weekly.nc\"),\n",
    "    \"gm_vpd\": pjoin(FEATURE_DIR, \"gridmet/vpd_1986_2020_weekly.nc\"),\n",
    "    \"aw_mat\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MAT.tif\"),\n",
    "    \"aw_mcmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MCMT.tif\"),\n",
    "    \"aw_mwmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MWMT.tif\"),\n",
    "    \"aw_td\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_TD.tif\"),\n",
    "    \"dm_tmax\": pjoin(FEATURE_DIR, \"daymet/tmax_1986_2020.nc\"),\n",
    "    \"dm_tmin\": pjoin(FEATURE_DIR, \"daymet/tmin_1986_2020.nc\"),\n",
    "    \"biomass_afg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_afg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"biomass_pfg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_pfg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"landfire_fvt\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FVT_200_CONUS/Tif/LC20_FVT_200.tif\"\n",
    "    ),\n",
    "    \"landfire_fbfm40\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FBFM40_200_CONUS/Tif/LC20_F40_200.tif\"\n",
    "    ),\n",
    "    \"ndvi\": pjoin(FEATURE_DIR, \"ndvi/access/weekly/ndvi_1986_2020_weekavg.nc\"),\n",
    "    \"mtbs_root\": pjoin(MTBS_DIR, \"MTBS_BSmosaics/\"),\n",
    "    \"mtbs_perim\": pjoin(MTBS_DIR, \"mtbs_perimeter_data/mtbs_perims_DD.shp\"),\n",
    "}\n",
    "YEARS = list(range(2016, 2021))\n",
    "GM_KEYS = list(filter(lambda x: x.startswith(\"gm_\"), PATHS))\n",
    "AW_KEYS = list(filter(lambda x: x.startswith(\"aw_\"), PATHS))\n",
    "DM_KEYS = list(filter(lambda x: x.startswith(\"dm_\"), PATHS))\n",
    "BIOMASS_KEYS = list(filter(lambda x: x.startswith(\"biomass_\"), PATHS))\n",
    "LANDFIRE_KEYS = list(filter(lambda x: x.startswith(\"landfire_\"), PATHS))\n",
    "NDVI_KEYS = list(filter(lambda x: x.startswith(\"ndvi\"), PATHS))\n",
    "DEM_KEYS = list(filter(lambda x: x.startswith(\"dem\"), PATHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out warnings from dask_geopandas and dask\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*initial implementation of Parquet.*\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Slicing is producing a large chunk.*\"\n",
    ")\n",
    "\n",
    "\n",
    "def hillshade(slope, aspect, azimuth=180, zenith=45):\n",
    "    # Convert angles from degrees to radians\n",
    "    azimuth_rad = np.radians(azimuth)\n",
    "    zenith_rad = np.radians(zenith)\n",
    "    slope_rad = np.radians(slope)\n",
    "    aspect_rad = np.radians(aspect)\n",
    "\n",
    "    # Calculate hillshade\n",
    "    shaded = np.sin(zenith_rad) * np.sin(slope_rad) + \\\n",
    "             np.cos(zenith_rad) * np.cos(slope_rad) * \\\n",
    "             np.cos(azimuth_rad - aspect_rad)\n",
    "    # scale to 0-255\n",
    "    shaded = 255 * (shaded + 1) / 2\n",
    "    # round hillshade to nearest integer\n",
    "    shaded = np.rint(shaded)\n",
    "    # convert to uint8\n",
    "    # Ensure non-finite values are not converted to int\n",
    "    # shaded = np.where(np.isfinite(shaded), shaded.astype(np.uint8), np.nan)\n",
    "    return shaded\n",
    "\n",
    "def hillshade_partition(df, zenith, azimuth):\n",
    "    # Apply the hillshade function to the slope and aspect columns\n",
    "    df['hillshade'] = hillshade(df['dem_slope'], df['dem_aspect'], azimuth, zenith)\n",
    "    return df\n",
    "\n",
    "def timestamp_to_year_part(df):\n",
    "    # Assuming 'ig_date' is the column with timestamp data\n",
    "    df['year'] = df['ig_date'].dt.year\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_nc_var_name(ds):\n",
    "    # Find the data variable in a nc xarray.Dataset\n",
    "    var_name = list(set(ds.keys()) - set([\"crs\", \"day_bnds\"]))[0]\n",
    "    # var_name = list(set(ds.keys()) - set([\"crs\", \"bnds\"]))[1] # for DAYMET ONLY!!\n",
    "    return var_name\n",
    "\n",
    "\n",
    "def netcdf_to_raster(path, date):\n",
    "    # This produces a Dataset. We need to grab the DataArray inside that\n",
    "    # contains the data of interest.\n",
    "    nc_ds = xr.open_dataset(path, chunks={\"day\": 1})#, decode_times=False)\n",
    "    nc_ds2 = nc_ds.drop_vars(\n",
    "        [\"latitude_bnds\", \"longitude_bnds\", \"time_bnds\"]\n",
    "    ).rio.write_crs(\"EPSG:5071\") # FOR NDVI ONLY!!\n",
    "    # nc_ds2 = nc_ds.rio.write_crs(\"EPSG:5071\")  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rio.write_crs(\n",
    "    #     nc_ds.coords[\"lambert_conformal_conic\"].spatial_ref\n",
    "    # )  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = nc_ds.rename({\"lambert_conformal_conic\": \"crs\"})  # FOR DAYMET ONLY!!\n",
    "    # nc_ds2 = nc_ds.drop_vars([\"lat\", \"lon\"])  # FOR DAYMET ONLY!!\n",
    "    # nc_ds = None # FOR DAYMET ONLY!!\n",
    "    # nc_ds2 = nc_ds2.rename_vars({\"x\": \"lon\", \"y\": \"lat\"})  # FOR DAYMET ONLY!!\n",
    "    # comment lines below for normal operation\n",
    "    #ds_crs = CRS.from_epsg(5071) dont need this line\n",
    "    #nc_ds.rio.write_crs(ds_crs) dont need this line\n",
    "    # nc_ds2 = nc_ds.rio.write_crs(nc_ds.crs.spatial_ref)\n",
    "    # nc_ds2 = nc_ds.rio.write_crs(nc_ds.crs) # for NDVI\n",
    "    # print nc_ds dimensions\n",
    "    # print(f\"{nc_ds.dims = }\")\n",
    "    # Find variable name\n",
    "    var_name = get_nc_var_name(nc_ds2)\n",
    "    # print(f\"var_name: {var_name}\")\n",
    "    # Extract\n",
    "    var_da = nc_ds2[var_name]\n",
    "    # print(f\"{var_da = }\")\n",
    "    var_da = var_da.sel(time=date, method=\"nearest\") # for DM and BM and NDVI\n",
    "    # var_da = var_da.sel(day=date, method=\"nearest\") # for GM\n",
    "    # xrs = xr.DataArray(\n",
    "    #     var_da.data, dims=(\"y\", \"x\"), coords=(var_da.lat.data, var_da.lon.data)\n",
    "    # ).expand_dims(\"band\") # For non-NDVI\n",
    "    xrs = xr.DataArray(\n",
    "        var_da.data, dims=(\"y\", \"x\"), coords=(var_da.latitude.data, var_da.longitude.data)\n",
    "    ).expand_dims(\"band\") # FOR NDVI ONLY!!\n",
    "    xrs[\"band\"] = [1]\n",
    "    # Set CRS in raster compliant format\n",
    "    xrs = xrs.rio.write_crs(nc_ds2.crs.spatial_ref)\n",
    "    return Raster(xrs)\n",
    "\n",
    "\n",
    "def extract_nc_data(df, nc_name):\n",
    "    assert df.ig_date.unique().size == 1\n",
    "    # print(f\"{gm_name}: {df.columns = }, {len(df) = }\")\n",
    "    date = df.ig_date.values[0]\n",
    "    print(f\"{nc_name}: starting {date}\")\n",
    "    rs = netcdf_to_raster(PATHS[nc_name], date)\n",
    "    bounds = gpd.GeoSeries(df.geometry).to_crs(rs.crs).total_bounds\n",
    "    rs = clipping.clip_box(rs, bounds)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .rename(columns={\"extracted\": nc_name})\n",
    "        .compute()\n",
    "    )\n",
    "    df[nc_name].values[:] = rdf[nc_name].values\n",
    "    # print(f\"{nc_name}: finished {date}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_state_dem_path(dem_key, state):\n",
    "    return pjoin(DEM_DATA_DIR, f\"{state}_{dem_key}.tif\")\n",
    "\n",
    "\n",
    "def extract_dem_data(df, key):\n",
    "    state = df.state.values[0]\n",
    "    path = get_state_dem_path(key, state)\n",
    "    rs = Raster(path)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .compute()\n",
    "    )\n",
    "    df[key].values[:] = rdf.extracted.values\n",
    "    return df\n",
    "\n",
    "def extract_tif_data(df, key):\n",
    "    state = df.state.values[0]\n",
    "    path = PATHS[key]\n",
    "    rs = Raster(path)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .compute()\n",
    "    )\n",
    "    df[key].values[:] = rdf.extracted.values\n",
    "    return df\n",
    "\n",
    "\n",
    "def partition_extract_nc(df, key):\n",
    "    # This func wraps extract_nc_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_nc_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_nc_data(gdf, key))\n",
    "    return pd.concat(parts)\n",
    "\n",
    "def partition_extract_tif(df, key):\n",
    "    # This func wraps extract_tif_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_tif_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_tif_data(gdf, key))\n",
    "    return pd.concat(parts)\n",
    "\n",
    "def clip_and_save_dem_rasters(keys, paths, feature, state):\n",
    "    feature = feature.compute()\n",
    "    for k in tqdm.tqdm(keys, ncols=80, desc=\"DEM Clipping\"):\n",
    "        path = paths[k]\n",
    "        out_path = get_state_dem_path(k, state)\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        rs = Raster(path)\n",
    "        (bounds,) = dask.compute(feature.to_crs(rs.crs).total_bounds)\n",
    "        print('dem crs: ', rs.crs)\n",
    "        crs = clipping.clip_box(rs, bounds)\n",
    "        crs.save(out_path)\n",
    "\n",
    "\n",
    "def build_mtbs_year_df(path, perims_df, state_label):\n",
    "    rs = Raster(path)\n",
    "    dfs = []\n",
    "    for grp in perims_df.groupby(\"Ig_Date\"):\n",
    "        date, perim = grp\n",
    "        df = (\n",
    "            clipping.clip(perim, rs)\n",
    "            .to_vector()\n",
    "            .rename(columns={\"value\": \"mtbs\"})\n",
    "            .drop(columns=[\"band\", \"row\", \"col\"])\n",
    "            .assign(state=state_label, ig_date=date)\n",
    "            .astype({\"mtbs\": U8})\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    print('perim crs: ', perims_df.crs)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "\n",
    "def _build_mtbs_df(\n",
    "    years, year_to_mtbs_file, year_to_perims, state, working_dir\n",
    "):\n",
    "    dfs = []\n",
    "    it = tqdm.tqdm(years, ncols=80, desc=\"MTBS\")\n",
    "    for y in it:\n",
    "        mtbs_path = year_to_mtbs_file[y]\n",
    "        if not os.path.exists(mtbs_path):\n",
    "            it.write(f\"No data for {y}\")\n",
    "            continue\n",
    "        perims = year_to_perims[y]\n",
    "        ydf = build_mtbs_year_df(mtbs_path, perims, state)\n",
    "        ypath = pjoin(working_dir, str(y))\n",
    "        ydf.compute().to_parquet(ypath)\n",
    "        ydf = dgpd.read_parquet(ypath)\n",
    "        dfs.append(ydf)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "\n",
    "def build_mtbs_df(\n",
    "    years, year_to_mtbs_file, year_to_perims, state, out_path, tmp_loc=TMP_LOC\n",
    "):\n",
    "    print(\"Building mtbs df\")\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        df = _build_mtbs_df(\n",
    "            years, year_to_mtbs_file, year_to_perims, state, working_dir\n",
    "        )\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)\n",
    "\n",
    "\n",
    "def add_columns_to_df(\n",
    "    df,\n",
    "    columns,\n",
    "    part_func,\n",
    "    out_path,\n",
    "    col_type=F32,\n",
    "    col_default=np.nan,\n",
    "    part_func_args=(),\n",
    "    tmp_loc=TMP_LOC,\n",
    "    parallel=True,\n",
    "):\n",
    "    print(f\"Adding columns: {columns}\")\n",
    "    # Add columns\n",
    "    expanded_df = df.assign(**{c: col_type.type(col_default) for c in columns})\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        # Save to disk before applying partition function. to_parquet() has a\n",
    "        # chance of segfaulting and that chance goes WAY up after adding\n",
    "        # columns and then mapping a function to partitions. Saving to disk\n",
    "        # before mapping keeps the odds low.\n",
    "        path = pjoin(working_dir, \"expanded\")\n",
    "        expanded_df.to_parquet(path)\n",
    "\n",
    "        expanded_df = dgpd.read_parquet(path)\n",
    "        meta = expanded_df._meta.copy()\n",
    "        for c in columns:\n",
    "            expanded_df = expanded_df.map_partitions(\n",
    "                part_func, c, *part_func_args, meta=meta\n",
    "            )\n",
    "\n",
    "        if parallel:\n",
    "            with ProgressBar():\n",
    "                expanded_df.to_parquet(out_path)\n",
    "        else:\n",
    "            # Save parts in serial and then assemble into single dataframe\n",
    "            with tempfile.TemporaryDirectory(dir=tmp_loc) as part_dir:\n",
    "                dfs = []\n",
    "                for i, part in enumerate(expanded_df.partitions):\n",
    "                    # Save part i\n",
    "                    part_path = pjoin(part_dir, f\"part{i:04}\")\n",
    "                    with ProgressBar():\n",
    "                        part.compute().to_parquet(part_path)\n",
    "                    # Save paths for opening with dask_geopandas later. Avoid\n",
    "                    # opening more dataframes in this loop as doing so will\n",
    "                    # likely cause a segfault. I have no idea why.\n",
    "                    dfs.append(part_path)\n",
    "                dfs = [dgpd.read_parquet(p) for p in dfs]\n",
    "                # Assemble and save to final output location\n",
    "                expanded_df = dd.concat(dfs)\n",
    "                with ProgressBar():\n",
    "                    expanded_df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_ds = xr.open_dataset(PATHS['ndvi'], chunks={\"day\": 1})#, decode_times=False)\n",
    "nc_ds2 = nc_ds.drop_vars(\n",
    "    [\"latitude_bnds\", \"longitude_bnds\", \"time_bnds\"]\n",
    ").rio.write_crs(\"EPSG:5071\") # FOR NDVI ONLY!!\n",
    "\n",
    "var_name = get_nc_var_name(nc_ds2)\n",
    "print(f\"var_name: {var_name}\")\n",
    "# Extract\n",
    "date='2020-08-16'\n",
    "var_da = nc_ds2[var_name]\n",
    "var_da = var_da.sel(time=date, method=\"nearest\") # for DM and BM and NDVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrs = xr.DataArray(\n",
    "        var_da.data, dims=(\"y\", \"x\"), coords=(var_da.latitude.data, var_da.longitude.data)\n",
    "    ).expand_dims(\"band\") # FOR NDVI ONLY!!\n",
    "xrs[\"band\"] = [1]\n",
    "# # Set CRS in raster compliant format\n",
    "xrs = xrs.rio.write_crs(nc_ds2.crs.spatial_ref)\n",
    "rs = Raster(xrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mtbs_df_2016_2020\n",
    "bounds = gpd.GeoSeries(df.geometry).to_crs(rs.crs).total_bounds\n",
    "rs = clipping.clip_box(rs, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: ESRI:102039>\n",
       "Name: USA_Contiguous_Albers_Equal_Area_Conic_USGS_version\n",
       "Axis Info [cartesian]:\n",
       "- [east]: Easting (metre)\n",
       "- [north]: Northing (metre)\n",
       "Area of Use:\n",
       "- undefined\n",
       "Coordinate Operation:\n",
       "- name: unnamed\n",
       "- method: Albers Equal Area\n",
       "Datum: North American Datum 1983\n",
       "- Ellipsoid: GRS 1980\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'crs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bounds \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49mGeoSeries(df\u001b[39m.\u001b[39;49mgeometry)\n\u001b[1;32m      2\u001b[0m bounds\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/geopandas/geoseries.py:222\u001b[0m, in \u001b[0;36mGeoSeries.__init__\u001b[0;34m(self, data, index, crs, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     name \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mname\n\u001b[1;32m    221\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(data, index\u001b[39m=\u001b[39mindex, name\u001b[39m=\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrs:\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrs \u001b[39m=\u001b[39m crs\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/geopandas/base.py:172\u001b[0m, in \u001b[0;36mGeoPandasBase.crs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcrs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    The Coordinate Reference System (CRS) represented as a ``pyproj.CRS``\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m    GeoSeries.to_crs : re-project to another CRS\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeometry\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mcrs\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'crs'"
     ]
    }
   ],
   "source": [
    "bounds = gpd.GeoSeries(df.geometry)\n",
    "bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # State borders\n",
    "# print(\"Loading state borders\")\n",
    "# stdf = open_vectors(PATHS[\"states\"], 0).data.to_crs(\"EPSG:5071\")\n",
    "# states = {st: stdf[stdf.STUSPS == st].geometry for st in list(stdf.STUSPS)}\n",
    "# state_shape = states[STATE]\n",
    "# states = None\n",
    "# stdf = None\n",
    "# # MTBS Perimeters\n",
    "# print(\"Loading MTBS perimeters\")\n",
    "# perimdf = open_vectors(PATHS[\"mtbs_perim\"]).data.to_crs(\"EPSG:5071\")\n",
    "# state_fire_perims = perimdf.clip(state_shape.compute())\n",
    "# state_fire_perims = (\n",
    "#     state_fire_perims.assign(\n",
    "#         Ig_Date=lambda frame: dd.to_datetime(\n",
    "#             frame.Ig_Date, format=\"%Y-%m-%d\"\n",
    "#         )\n",
    "#     )\n",
    "#     .sort_values(\"Ig_Date\")\n",
    "#     .compute()\n",
    "# )\n",
    "# year_to_perims = {\n",
    "#     y: state_fire_perims[state_fire_perims.Ig_Date.dt.year == y]\n",
    "#     for y in YEARS\n",
    "# }\n",
    "# state_fire_perims = None\n",
    "# year_to_mtbs_file = {\n",
    "#     y: pjoin(PATHS[\"mtbs_root\"], f\"mtbs_{STATE}_{y}.tif\")\n",
    "#     for y in YEARS\n",
    "# }\n",
    "# print(year_to_mtbs_file)\n",
    "mtbs_df_path = pjoin(TMP_LOC, f\"{STATE}_mtbs.parquet\")\n",
    "mtbs_df_temp_path = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp.parquet\")\n",
    "checkpoint_1_path = pjoin(TMP_LOC, \"check1\")\n",
    "checkpoint_2_path = pjoin(TMP_LOC, \"check2\")\n",
    "if 0:\n",
    "    # code below for creating a new dataset for a new state / region\n",
    "    df = build_mtbs_df(\n",
    "        YEARS,\n",
    "        year_to_mtbs_file,\n",
    "        year_to_perims,\n",
    "        STATE,\n",
    "        out_path=checkpoint_1_path,\n",
    "    )\n",
    "    # df = add_columns_to_df(\n",
    "    #     df, GM_KEYS, partition_extract_gridmet, checkpoint_1_path\n",
    "    # )\n",
    "    clip_and_save_dem_rasters(DEM_KEYS, PATHS, state_shape, STATE)\n",
    "    df = add_columns_to_df(\n",
    "        df,\n",
    "        DEM_KEYS,\n",
    "        extract_dem_data,\n",
    "        checkpoint_1_path,\n",
    "        # Save results in serial to avoid segfaulting. Something about the\n",
    "        # dem computations makes segfaults extremely likely when saving\n",
    "        # The computations require a lot of memory which may be what\n",
    "        # triggers the fault.\n",
    "        parallel=False,\n",
    "    )\n",
    "    df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "    print(\"Repartitioning\")\n",
    "    with ProgressBar():\n",
    "        df.to_parquet(checkpoint_2_path)\n",
    "if 1:\n",
    "    # code below used to add new features to the dataset\n",
    "    with ProgressBar():\n",
    "        df = dgpd.read_parquet(checkpoint_2_path)\n",
    "    df = add_columns_to_df(\n",
    "        df, NDVI_KEYS, partition_extract_nc, checkpoint_1_path, parallel=False\n",
    "    ) # for NetCDF data\n",
    "    # df = add_columns_to_df(\n",
    "    #     df, LANDFIRE_KEYS, partition_extract_tif, checkpoint_1_path, parallel=False\n",
    "    # ) # for TIF data\n",
    "    df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "    print(\"Repartitioning\")\n",
    "    with ProgressBar():\n",
    "        df.to_parquet(checkpoint_1_path)\n",
    "if 0:\n",
    "    with ProgressBar():\n",
    "        df = dgpd.read_parquet(mtbs_df_temp_path)\n",
    "    # df = df.assign(hillshade=U8.type(0))\n",
    "    # df = df.map_partitions(hillshade_partition, 45, 180, meta=df._meta)\n",
    "    # df = df.assign(year=U16.type(0))\n",
    "    # df = df.map_partitions(timestamp_to_year_part, meta=df._meta)\n",
    "    print(df.head())\n",
    "    print(\"Repartitioning and saving \")\n",
    "    df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "    # df = df.assign(unique_id=str) does not work, added outside of dask\n",
    "    # df = df.map_partitions(add_unique_identifier, meta=df._meta)\n",
    "    with ProgressBar():\n",
    "        # df.to_parquet(mtbs_df_temp_path)\n",
    "        df.to_parquet(mtbs_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020 = dgpd.read_parquet(checkpoint_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020_computed_dem_only = mtbs_df_2016_2020.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MTBS DATASET ADD UID AND EVENT ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dask_df = dgpd.read_parquet(mtbs_df_temp_path)\n",
    "mtbs_dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset = mtbs_dask_df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_2016_2020 = mtbs_dataset[mtbs_dataset.ig_date.dt.year >= 2016]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020_computed_dem_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_df_2016_2020_computed_dem_only[mtbs_df_2016_2020_computed_dem_only['ig_date'] == '2020-08-16'].plot(column='dem', cmap='terrain', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_2016_2020[mtbs_dataset_2016_2020['ig_date'] == '2020-08-16'].plot(column='dem', cmap='terrain', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adds unique id to each pixel\n",
    "# mtbs_dataset.reset_index(inplace=True)\n",
    "# mtbs_dataset['index'] = mtbs_dataset.index\n",
    "# # rename index to unique_id\n",
    "# mtbs_dataset.rename(columns={'index': 'unique_id'}, inplace=True)\n",
    "# mtbs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rewrite to parquet\n",
    "# mtbs_dataset.to_parquet(mtbs_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # a function to round numeric values in a dataframe to 3 decimals (if they are > 1)\n",
    "# def round_df(df):\n",
    "#     columns_to_round = ['aw_mwmt', 'dm_tmax', 'dm_tmin', 'gm_srad', 'gm_vpd', 'aw_td', 'aw_mcmt', 'dem_aspect', 'dem_slope']\n",
    "#     for col in columns_to_round:\n",
    "#         df[col] = df[col].round(3)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mtbs perimeters\n",
    "print(\"Loading MTBS perimeters\")\n",
    "mtbs_perim = gpd.read_file(PATHS[\"mtbs_perim\"])\n",
    "mtbs_perim['Ig_Date'] = pd.to_datetime(mtbs_perim['Ig_Date'])\n",
    "mtbs_perim.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only the columns we need (Event_ID where startswith OR, Ig_Date, and geometry)\n",
    "mtbs_perim = mtbs_perim[[\"Event_ID\", \"Ig_Date\", \"geometry\"]]\n",
    "mtbs_perim = mtbs_perim[mtbs_perim.Event_ID.str.startswith(\"OR\")]\n",
    "# drop rows where Ig_Date before 1986 or after 2020\n",
    "mtbs_perim = mtbs_perim[mtbs_perim.Ig_Date.dt.year.between(1986, 2020)]\n",
    "mtbs_perim.reset_index(drop=True, inplace=True)\n",
    "len(mtbs_perim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtbs_dataset['Fire_ID'] = 'None'\n",
    "# mtbs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fire_perim_geom_OR4310211883919860320 = mtbs_perim[mtbs_perim.Ig_Date.dt.year == 1986].loc[40, 'geometry']\n",
    "# fire_ig_date_OR4310211883919860320 = mtbs_perim[mtbs_perim.Ig_Date.dt.year == 1986].loc[40, 'Ig_Date']\n",
    "# fire_perim_geom_OR4310211883919860320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_perim.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dask_df.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the geometry in mtbs_dataset to lat lon\n",
    "# mtbs_dask_df = mtbs_dask_df.to_crs(epsg=4326)\n",
    "mtbs_perim = mtbs_perim.to_crs(mtbs_dask_df.crs)\n",
    "# mtbs_dataset = mtbs_dataset.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_join(partition, mtbs_perim):\n",
    "    # Convert the Dask partition to a GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(partition, geometry='geometry', crs=mtbs_perim.crs)\n",
    "\n",
    "    # Perform the spatial join\n",
    "    joined = gpd.sjoin(gdf, mtbs_perim, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "    # Filter by date if needed\n",
    "    joined = joined[joined['ig_date'] == joined['Ig_Date']]\n",
    "    # print(joined)\n",
    "\n",
    "    return joined\n",
    "\n",
    "result = mtbs_dask_df.map_partitions(spatial_join, mtbs_perim, align_dataframes=False)\n",
    "with ProgressBar():\n",
    "    final_result = result.compute()\n",
    "print(len(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compare final_results and mtbs_dataset to see what is missing from final_results after the spatial join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.drop(columns=['index_right', 'Ig_Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "final_result.dropna(inplace=True)\n",
    "# adds unique id to each pixel\n",
    "final_result.reset_index(inplace=True)\n",
    "final_result['index'] = final_result.index\n",
    "# rename index to unique_id\n",
    "final_result.rename(columns={'index': 'unique_id'}, inplace=True)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write final_result to dask so we can write to parquet\n",
    "# final_result = dd.from_pandas(final_result, npartitions=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    final_result.to_parquet(mtbs_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and check\n",
    "mtbs_dataset_final = dgpd.read_parquet(mtbs_df_path)\n",
    "mtbs_dataset_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    mtbs_final_computed = mtbs_dataset_final.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a fire topo map\n",
    "\n",
    "eventID = 'OR4380511789220200816'\n",
    "\n",
    "fire = final_result[final_result.Event_ID == eventID]\n",
    "\n",
    "col = 'dem'\n",
    "\n",
    "geo_mask = fire.geometry.unary_union\n",
    "date_mask = fire.ig_date\n",
    "\n",
    "fire.plot(column=col, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_2020_08_16 = mtbs_dataset[(mtbs_dataset.year == 2020) & (mtbs_dataset.ig_date.dt.month == 8) & (mtbs_dataset.ig_date.dt.day == 16)]\n",
    "mtbs_dataset_2020_08_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_dataset_2020_08_16.plot(column=col, s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data info\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtbs_data_fixed = dgpd.read_parquet(mtbs_df_path)\n",
    "mtbs_data_fixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from mtbs_df_path\n",
    "mtbs_data = dgpd.read_parquet(mtbs_df_temp_path)\n",
    "mtbs_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    mtbs_full = mtbs_dataset.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dem = need to explore dem and see why its is all scrambled (perhaps from mtbs builder projection changes?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gisenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
