{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import warnings\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy.fft import dst\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from raster_tools import Raster, Vector, open_vectors, clipping, zonal\n",
    "from raster_tools.dtypes import F32, U8, U16\n",
    "\n",
    "# Filter out warnings from dask_geopandas and dask\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*initial implementation of Parquet.*\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Slicing is producing a large chunk.*\"\n",
    ")\n",
    "\n",
    "\n",
    "# Location for temporary storage\n",
    "TMP_LOC = \"/home/jake/FireLab/Project/data/temp/\"\n",
    "TMP_LOC2 = \"/home/jake/FireLab/Project/data/temp2/\"\n",
    "DATA_LOC = \"/home/jake/FireLab/Project/data/\"\n",
    "\n",
    "STATE = \"OR\"\n",
    "\n",
    "# Location of clipped DEM files\n",
    "DEM_DATA_DIR = pjoin(TMP_LOC, \"dem_data\")\n",
    "\n",
    "# location of feature data files\n",
    "FEATURE_DIR = pjoin(DATA_LOC, \"FeatureData\")\n",
    "EDNA_DIR = pjoin(DATA_LOC, \"terrain\")\n",
    "MTBS_DIR = pjoin(DATA_LOC, \"MTBS_Data\")\n",
    "\n",
    "PATHS = {\n",
    "    \"states\": pjoin(EDNA_DIR, \"state_borders/cb_2018_us_state_5m.shp\"),\n",
    "    \"dem\": pjoin(EDNA_DIR, \"us_orig_dem/us_orig_dem/orig_dem/hdr.adf\"),\n",
    "    \"dem_slope\": pjoin(EDNA_DIR, \"us_slope/us_slope/slope/hdr.adf\"),\n",
    "    \"dem_aspect\": pjoin(EDNA_DIR, \"us_aspect/aspect/hdr.adf\"),\n",
    "    \"dem_flow_acc\": pjoin(EDNA_DIR, \"us_flow_acc/us_flow_acc/flow_acc/hdr.adf\"),\n",
    "    \"gm_srad\": pjoin(FEATURE_DIR, \"gridmet/srad_1986_2020_weekly.nc\"),\n",
    "    \"gm_vpd\": pjoin(FEATURE_DIR, \"gridmet/vpd_1986_2020_weekly.nc\"),\n",
    "    \"aw_mat\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MAT.tif\"),\n",
    "    \"aw_mcmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MCMT.tif\"),\n",
    "    \"aw_mwmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MWMT.tif\"),\n",
    "    \"aw_td\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_TD.tif\"),\n",
    "    \"dm_tmax\": pjoin(FEATURE_DIR, \"daymet/tmax_1986_2020.nc\"),\n",
    "    \"dm_tmin\": pjoin(FEATURE_DIR, \"daymet/tmin_1986_2020.nc\"),\n",
    "    \"biomass_afg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_afg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"biomass_pfg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_pfg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"landfire_fvt\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FVT_200_CONUS/Tif/LC20_FVT_200.tif\"\n",
    "    ),\n",
    "    \"landfire_fbfm40\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FBFM40_200_CONUS/Tif/LC20_F40_200.tif\"\n",
    "    ),\n",
    "    \"ndvi\": pjoin(FEATURE_DIR, \"ndvi/access/weekly/ndvi_1986_2020_weekavg.nc\"),\n",
    "    \"mtbs_root\": pjoin(MTBS_DIR, \"MTBS_BSmosaics/\"),\n",
    "    \"mtbs_perim\": pjoin(MTBS_DIR, \"mtbs_perimeter_data/mtbs_perims_DD.shp\"),\n",
    "}\n",
    "YEARS = list(range(2018, 2021))\n",
    "GM_KEYS = list(filter(lambda x: x.startswith(\"gm_\"), PATHS)) # added\n",
    "AW_KEYS = list(filter(lambda x: x.startswith(\"aw_\"), PATHS))\n",
    "DM_KEYS = list(filter(lambda x: x.startswith(\"dm_\"), PATHS)) # added\n",
    "BIOMASS_KEYS = list(filter(lambda x: x.startswith(\"biomass_\"), PATHS)) # added\n",
    "LANDFIRE_KEYS = list(filter(lambda x: x.startswith(\"landfire_\"), PATHS))\n",
    "NDVI_KEYS = list(filter(lambda x: x.startswith(\"ndvi\"), PATHS)) # added\n",
    "DEM_KEYS = list(filter(lambda x: x.startswith(\"dem\"), PATHS)) # added\n",
    "\n",
    "# NC_KEYSET = set(GM_KEYS + DM_KEYS + BIOMASS_KEYS + NDVI_KEYS)\n",
    "NC_KEYSET = [DM_KEYS, GM_KEYS, BIOMASS_KEYS, NDVI_KEYS]\n",
    "TIF_KEYSET = [AW_KEYS, LANDFIRE_KEYS]\n",
    "\n",
    "MTBS_DF_PATH = pjoin(TMP_LOC, f\"{STATE}_mtbs.parquet\")\n",
    "MTBS_DF_PARQUET_PATH_NEW = pjoin(TMP_LOC, f\"{STATE}_mtbs_new.parquet\")\n",
    "MTBS_DF_TEMP_PATH = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp.parquet\")\n",
    "MTBS_DF_TEMP_PATH_2 = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp_2.parquet\")\n",
    "CHECKPOINT_1_PATH = pjoin(TMP_LOC, \"check1\")\n",
    "CHECKPOINT_2_PATH = pjoin(TMP_LOC, \"check2\")\n",
    "\n",
    "\n",
    "def hillshade(slope, aspect, azimuth=180, zenith=45):\n",
    "    # Convert angles from degrees to radians\n",
    "    azimuth_rad = np.radians(azimuth)\n",
    "    zenith_rad = np.radians(zenith)\n",
    "    slope_rad = np.radians(slope)\n",
    "    aspect_rad = np.radians(aspect)\n",
    "\n",
    "    # Calculate hillshade\n",
    "    shaded = np.sin(zenith_rad) * np.sin(slope_rad) + \\\n",
    "             np.cos(zenith_rad) * np.cos(slope_rad) * \\\n",
    "             np.cos(azimuth_rad - aspect_rad)\n",
    "    # scale to 0-255\n",
    "    shaded = 255 * (shaded + 1) / 2\n",
    "    # round hillshade to nearest integer\n",
    "    shaded = np.rint(shaded)\n",
    "    # convert to uint8\n",
    "    # Ensure non-finite values are not converted to int\n",
    "    # shaded = np.where(np.isfinite(shaded), shaded.astype(np.uint8), np.nan)\n",
    "    return shaded\n",
    "\n",
    "def hillshade_partition(df, zenith, azimuth):\n",
    "    # Apply the hillshade function to the slope and aspect columns\n",
    "    df['hillshade'] = hillshade(df['dem_slope'], df['dem_aspect'], azimuth, zenith)\n",
    "    return df\n",
    "\n",
    "def timestamp_to_year_part(df):\n",
    "    # Assuming 'ig_date' is the column with timestamp data\n",
    "    df['year'] = df['ig_date'].dt.year\n",
    "    return df\n",
    "\n",
    "def get_nc_var_name(ds, nc_feat_name):\n",
    "    # Find the data variable in a nc xarray.Dataset\n",
    "    if nc_feat_name.startswith(\"dm\"):\n",
    "        var_name = list(set(ds.keys()) - set([\"crs\", \"bnds\"]))[0] # for DAYMET ONLY!!\n",
    "    else:\n",
    "        var_name = list(set(ds.keys()) - set([\"crs\", \"day_bnds\"]))[0]\n",
    "    return var_name\n",
    "\n",
    "\n",
    "def netcdf_to_raster(path, date, nc_feature_name):\n",
    "    # This produces a Dataset. We need to grab the DataArray inside that\n",
    "    # contains the data of interest.\n",
    "    nc_ds = xr.open_dataset(path, chunks={\"day\": 1})#, decode_times=False)\n",
    "    if nc_feature_name == \"ndvi\":\n",
    "        nc_ds2 = nc_ds.drop_vars(\n",
    "        [\"latitude_bnds\", \"longitude_bnds\", \"time_bnds\"]\n",
    "        ).rio.write_crs(\"EPSG:5071\") # FOR NDVI ONLY!!\n",
    "    elif nc_feature_name.startswith(\"dm_\"):\n",
    "        nc_ds2 = nc_ds.rio.write_crs(\"EPSG:5071\")  # FOR DAYMET ONLY!!\n",
    "        # nc_ds = nc_ds.rio.write_crs(\n",
    "        #     nc_ds.coords[\"lambert_conformal_conic\"].spatial_ref\n",
    "        # )  # FOR DAYMET ONLY!!\n",
    "        nc_ds2 = nc_ds2.rename({\"lambert_conformal_conic\": \"crs\"})  # FOR DAYMET ONLY!!\n",
    "        nc_ds2 = nc_ds2.drop_vars([\"lat\", \"lon\", \"time_bnds\"])  # FOR DAYMET ONLY!!\n",
    "        nc_ds = None # FOR DAYMET ONLY!!\n",
    "        nc_ds2 = nc_ds2.rename_vars({\"x\": \"lon\", \"y\": \"lat\"})  # FOR DAYMET ONLY!!\n",
    "    else:\n",
    "        nc_ds2 = nc_ds.rio.write_crs(\"EPSG:5071\")\n",
    "\n",
    "    # Find variable name\n",
    "    var_name = get_nc_var_name(nc_ds2, nc_feature_name)\n",
    "    # print(f\"var_name: {var_name}\")\n",
    "    # Extract\n",
    "    var_da = nc_ds2[var_name]\n",
    "    # print(f\"{var_da = }\")\n",
    "    if nc_feature_name.startswith(\"gm_\"):\n",
    "        var_da = var_da.sel(day=date, method=\"nearest\") # for GM\n",
    "    else:\n",
    "        var_da = var_da.sel(time=date, method=\"nearest\") # for DM and BM and NDVI\n",
    "    if nc_feature_name.startswith(\"ndvi\"):\n",
    "        xrs = xr.DataArray(\n",
    "            var_da.data, dims=(\"y\", \"x\"), coords=(var_da.latitude.data, var_da.longitude.data)\n",
    "        ).expand_dims(\"band\") # FOR NDVI ONLY!!\n",
    "    else:\n",
    "        xrs = xr.DataArray(\n",
    "            var_da.data, dims=(\"y\", \"x\"), coords=(var_da.lat.data, var_da.lon.data)\n",
    "        ).expand_dims(\"band\") # For non-NDVI\n",
    "    xrs[\"band\"] = [1]\n",
    "    # Set CRS in raster compliant format\n",
    "    xrs = xrs.rio.write_crs(nc_ds2.crs.spatial_ref)\n",
    "    return Raster(xrs)\n",
    "\n",
    "\n",
    "def extract_nc_data(df, nc_name):\n",
    "    assert df.ig_date.unique().size == 1\n",
    "    # print(f\"{gm_name}: {df.columns = }, {len(df) = }\")\n",
    "    date = df.ig_date.values[0]\n",
    "    print(f\"{nc_name}: starting {date}\")\n",
    "    rs = netcdf_to_raster(PATHS[nc_name], date, nc_name)\n",
    "    bounds = gpd.GeoSeries(df.geometry).to_crs(rs.crs).total_bounds\n",
    "    rs = clipping.clip_box(rs, bounds)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .rename(columns={\"extracted\": nc_name})\n",
    "        .compute()\n",
    "    )\n",
    "    df[nc_name].values[:] = rdf[nc_name].values\n",
    "    # print(f\"{nc_name}: finished {date}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_state_dem_path(dem_key, state):\n",
    "    return pjoin(DEM_DATA_DIR, f\"{state}_{dem_key}.tif\")\n",
    "\n",
    "\n",
    "def extract_dem_data(df, key):\n",
    "    state = df.state.values[0]\n",
    "    path = get_state_dem_path(key, state)\n",
    "    rs = Raster(path)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .compute()\n",
    "    )\n",
    "    df[key].values[:] = rdf.extracted.values\n",
    "    return df\n",
    "\n",
    "def extract_tif_data(df, key):\n",
    "    state = df.state.values[0]\n",
    "    path = PATHS[key]\n",
    "    rs = Raster(path)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .compute()\n",
    "    )\n",
    "    df[key].values[:] = rdf.extracted.values\n",
    "    return df\n",
    "\n",
    "\n",
    "def partition_extract_nc(df, key):\n",
    "    # This func wraps extract_nc_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_nc_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_nc_data(gdf, key))\n",
    "    return pd.concat(parts)\n",
    "\n",
    "def partition_extract_tif(df, key):\n",
    "    # This func wraps extract_tif_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_tif_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_tif_data(gdf, key))\n",
    "    return pd.concat(parts)\n",
    "\n",
    "def clip_and_save_dem_rasters(keys, paths, feature, state):\n",
    "    feature = feature.compute()\n",
    "    for k in tqdm.tqdm(keys, ncols=80, desc=\"DEM Clipping\"):\n",
    "        path = paths[k]\n",
    "        out_path = get_state_dem_path(k, state)\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        rs = Raster(path)\n",
    "        (bounds,) = dask.compute(feature.to_crs(rs.crs).total_bounds)\n",
    "        crs = clipping.clip_box(rs, bounds)\n",
    "        crs.save(out_path)\n",
    "\n",
    "\n",
    "def build_mtbs_year_df(perims_df, state_label):\n",
    "    dfs = []\n",
    "    for grp in perims_df.groupby(\"Ig_Date\"):\n",
    "        date, perim = grp\n",
    "        perim = perim.copy()\n",
    "        perim[\"ig_date\"] = date\n",
    "        perim[\"state\"] = state_label\n",
    "        perim[\"fireid\"] = perim[\"fireid\"].astype(str)\n",
    "        dfs.append(perim)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "\n",
    "def _build_mtbs_df(\n",
    "    years, year_to_perims, state, working_dir\n",
    "):\n",
    "    dfs = []\n",
    "    it = tqdm.tqdm(years, ncols=80, desc=\"MTBS\")\n",
    "    for y in it:\n",
    "        # mtbs_path = year_to_mtbs_file[y]\n",
    "        # if not os.path.exists(mtbs_path):\n",
    "        #     it.write(f\"No data for {y}\")\n",
    "        #     continue\n",
    "        perims = year_to_perims[y]\n",
    "        ydf = build_mtbs_year_df(perims, state)\n",
    "        ypath = pjoin(working_dir, str(y))\n",
    "        ydf.compute().to_parquet(ypath)\n",
    "        ydf = dgpd.read_parquet(ypath)\n",
    "        dfs.append(ydf)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "\n",
    "def build_mtbs_df(\n",
    "    years, year_to_perims, state, out_path, tmp_loc=TMP_LOC\n",
    "):\n",
    "    print(\"Building mtbs df\")\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        df = _build_mtbs_df(\n",
    "            years, year_to_perims, state, working_dir\n",
    "        )\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)\n",
    "\n",
    "\n",
    "def add_columns_to_df(\n",
    "    df,\n",
    "    columns,\n",
    "    part_func,\n",
    "    out_path,\n",
    "    col_type=F32,\n",
    "    col_default=np.nan,\n",
    "    part_func_args=(),\n",
    "    tmp_loc=TMP_LOC,\n",
    "    parallel=True,\n",
    "):\n",
    "    print(f\"Adding columns: {columns}\")\n",
    "    # Add columns\n",
    "    expanded_df = df.assign(**{c: col_type.type(col_default) for c in columns})\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        # Save to disk before applying partition function. to_parquet() has a\n",
    "        # chance of segfaulting and that chance goes WAY up after adding\n",
    "        # columns and then mapping a function to partitions. Saving to disk\n",
    "        # before mapping keeps the odds low.\n",
    "        path = pjoin(working_dir, \"expanded\")\n",
    "        expanded_df.to_parquet(path)\n",
    "\n",
    "        expanded_df = dgpd.read_parquet(path)\n",
    "        meta = expanded_df._meta.copy()\n",
    "        for c in columns:\n",
    "            expanded_df = expanded_df.map_partitions(\n",
    "                part_func, c, *part_func_args, meta=meta\n",
    "            )\n",
    "\n",
    "        if parallel:\n",
    "            with ProgressBar():\n",
    "                expanded_df.to_parquet(out_path)\n",
    "        else:\n",
    "            # Save parts in serial and then assemble into single dataframe\n",
    "            with tempfile.TemporaryDirectory(dir=tmp_loc) as part_dir:\n",
    "                dfs = []\n",
    "                for i, part in enumerate(expanded_df.partitions):\n",
    "                    # Save part i\n",
    "                    part_path = pjoin(part_dir, f\"part{i:04}\")\n",
    "                    with ProgressBar():\n",
    "                        part.compute().to_parquet(part_path)\n",
    "                    # Save paths for opening with dask_geopandas later. Avoid\n",
    "                    # opening more dataframes in this loop as doing so will\n",
    "                    # likely cause a segfault. I have no idea why.\n",
    "                    dfs.append(part_path)\n",
    "                dfs = [dgpd.read_parquet(p) for p in dfs]\n",
    "                # Assemble and save to final output location\n",
    "                expanded_df = dd.concat(dfs)\n",
    "                with ProgressBar():\n",
    "                    expanded_df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state borders\n",
      "Loading MTBS perimeters\n",
      "Loading VIIRS perimeters\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /home/jake/FireLab/Project/data/viirs_perims.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py:543\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, columns, filters, categories, index, storage_options, engine, use_nullable_dtypes, dtype_backend, calculate_divisions, ignore_metadata_file, metadata_task_size, split_row_groups, blocksize, aggregate_files, parquet_file_extension, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     blocksize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m read_metadata_result \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalculate_divisions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# In the future, we may want to give the engine the\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# option to return a dedicated element for `common_kwargs`.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# However, to avoid breaking the API, we just embed this\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# data in the first element of `parts` for now.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# The logic below is inteded to handle backward and forward\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# compatibility with a user-defined engine.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask_geopandas/io/parquet.py:57\u001b[0m, in \u001b[0;36mGeoArrowEngine.read_metadata\u001b[0;34m(cls, fs, paths, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_metadata\u001b[39m(\u001b[38;5;28mcls\u001b[39m, fs, paths, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 57\u001b[0m     meta, stats, parts, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     gather_spatial_partitions \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgather_spatial_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:534\u001b[0m, in \u001b[0;36mArrowDatasetEngine.read_metadata\u001b[0;34m(cls, fs, paths, categories, index, use_nullable_dtypes, dtype_backend, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# Stage 1: Collect general dataset information\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect_dataset_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_statistics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_row_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregate_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_metadata_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_task_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparquet_file_extension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Stage 2: Generate output `meta`\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask/dataframe/io/parquet/arrow.py:1049\u001b[0m, in \u001b[0;36mArrowDatasetEngine._collect_dataset_info\u001b[0;34m(cls, paths, fs, categories, index, gather_statistics, filters, split_row_groups, blocksize, aggregate_files, ignore_metadata_file, metadata_task_size, parquet_file_extension, kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mpa_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_wrapped_fs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_processed_dataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Get file_frag sample and extract physical_schema\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pyarrow/dataset.py:765\u001b[0m, in \u001b[0;36mdataset\u001b[0;34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_path_like(elem) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(elem, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m source):\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pyarrow/dataset.py:443\u001b[0m, in \u001b[0;36m_filesystem_dataset\u001b[0;34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 443\u001b[0m     fs, paths_or_selector \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_multiple_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/pyarrow/dataset.py:362\u001b[0m, in \u001b[0;36m_ensure_multiple_sources\u001b[0;34m(paths, filesystem)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mNotFound:\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(info\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file_type \u001b[38;5;241m==\u001b[39m FileType\u001b[38;5;241m.\u001b[39mDirectory:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /home/jake/FireLab/Project/data/viirs_perims.parquet",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# perimdf = open_vectors(PATHS[\"mtbs_perim\"]).data.to_crs(\"EPSG:5071\")\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading VIIRS perimeters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     perimdf \u001b[38;5;241m=\u001b[39m \u001b[43mdgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_LOC\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mviirs_perims.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute()\u001b[38;5;241m.\u001b[39mto_crs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPSG:5071\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# perimdf = perimdf.rename(columns={\"t\": \"Ig_Date\"})\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# state_fire_perims = perimdf.clip(state_shape.compute())\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# state_fire_perims = (\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# print(year_to_mtbs_file)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# code below for creating a new dataset for a new state / region\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask_geopandas/io/parquet.py:112\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGeoArrowEngine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# check if spatial partitioning information was stored\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     spatial_partitions \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/firelab/lib/python3.9/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_parquet method registered to the pandas backend.\nOriginal Message: /home/jake/FireLab/Project/data/viirs_perims.parquet"
     ]
    }
   ],
   "source": [
    "### MAIN ###\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if 1:\n",
    "        # State borders\n",
    "        print(\"Loading state borders\")\n",
    "        stdf = open_vectors(PATHS[\"states\"], 0).data.to_crs(\"EPSG:5071\")\n",
    "        states = {st: stdf[stdf.STUSPS == st].geometry for st in list(stdf.STUSPS)}\n",
    "        state_shape = states[STATE]\n",
    "        states = None\n",
    "        stdf = None\n",
    "\n",
    "        # MTBS Perimeters\n",
    "        print(\"Loading MTBS perimeters\")\n",
    "        perimdf = open_vectors(PATHS[\"mtbs_perim\"]).data.to_crs(\"EPSG:5071\")\n",
    "        # print(\"Loading VIIRS perimeters\")\n",
    "        # perimdf = dgpd.read_parquet(DATA_LOC + \"viirs_perims.parquet\").compute().to_crs(\"EPSG:5071\")\n",
    "        # perimdf = perimdf.rename(columns={\"t\": \"Ig_Date\"})\n",
    "        state_fire_perims = perimdf.clip(state_shape.compute())\n",
    "        state_fire_perims = (\n",
    "            state_fire_perims.assign(\n",
    "                Ig_Date=lambda frame: dd.to_datetime(\n",
    "                    frame.Ig_Date, format=\"%Y-%m-%d\"\n",
    "                )\n",
    "            )\n",
    "            .sort_values(\"Ig_Date\")\n",
    "            .compute()\n",
    "        )\n",
    "        state_fire_perims = state_fire_perims[state_fire_perims.Ig_Date.dt.year.between(2018, 2020)]\n",
    "        year_to_perims = {\n",
    "            y: state_fire_perims[state_fire_perims.Ig_Date.dt.year == y]\n",
    "            for y in YEARS\n",
    "        }\n",
    "        state_fire_perims = None\n",
    "\n",
    "        year_to_mtbs_file = {\n",
    "            y: pjoin(PATHS[\"mtbs_root\"], f\"mtbs_{STATE}_{y}.tif\")\n",
    "            for y in YEARS\n",
    "        }\n",
    "\n",
    "        # print(year_to_mtbs_file)\n",
    "\n",
    "\n",
    "    if 0:\n",
    "        # code below for creating a new dataset for a new state / region\n",
    "        df = build_mtbs_df(\n",
    "            YEARS,\n",
    "            year_to_mtbs_file,\n",
    "            year_to_perims,\n",
    "            STATE,\n",
    "            out_path=MTBS_DF_TEMP_PATH_2,\n",
    "        )\n",
    "        clip_and_save_dem_rasters(DEM_KEYS, PATHS, state_shape, STATE)\n",
    "        df = add_columns_to_df(\n",
    "            df,\n",
    "            DEM_KEYS,\n",
    "            extract_dem_data,\n",
    "            CHECKPOINT_1_PATH,\n",
    "            # Save results in serial to avoid segfaulting. Something about the\n",
    "            # dem computations makes segfaults extremely likely when saving\n",
    "            # The computations require a lot of memory which may be what\n",
    "            # triggers the fault.\n",
    "            parallel=False,\n",
    "        )\n",
    "        df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "        print(\"Repartitioning\")\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(CHECKPOINT_2_PATH)\n",
    "        df = None\n",
    "\n",
    "    if 0:\n",
    "        # code below used to add new features to the dataset\n",
    "        with ProgressBar():\n",
    "            df = dgpd.read_parquet(MTBS_DF_PARQUET_PATH_NEW)\n",
    "\n",
    "        # loop to add all nc features\n",
    "        for nc_name in NC_KEYSET:\n",
    "            print(f\"Adding {nc_name}\")\n",
    "            df = add_columns_to_df(\n",
    "                df, nc_name, partition_extract_nc, CHECKPOINT_1_PATH, parallel=False\n",
    "            )\n",
    "        # loop to add all tif features\n",
    "        # for tif_name in TIF_KEYSET:\n",
    "        #     print(f\"Adding {tif_name}\")\n",
    "        #     df = add_columns_to_df(\n",
    "        #         df, [tif_name], partition_extract_tif, CHECKPOINT_1_PATH, parallel=False\n",
    "        #     )\n",
    "        # add hillshade and year columns\n",
    "        df_meta = df._meta.copy()\n",
    "        df = df.assign(hillshade=U8.type(0))\n",
    "        df = df.map_partitions(hillshade_partition, 45, 180, meta=df_meta)\n",
    "        df = df.assign(year=U16.type(0))\n",
    "        df = df.map_partitions(timestamp_to_year_part, meta=df_meta)\n",
    "        print(\"Columns: \", df.columns)\n",
    "        # df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "        print(\"Repartitioning\")\n",
    "        df = df.compute()\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(MTBS_DF_TEMP_PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_ID</th>\n",
       "      <th>irwinID</th>\n",
       "      <th>Incid_Name</th>\n",
       "      <th>Incid_Type</th>\n",
       "      <th>Map_ID</th>\n",
       "      <th>Map_Prog</th>\n",
       "      <th>Asmnt_Type</th>\n",
       "      <th>BurnBndAc</th>\n",
       "      <th>BurnBndLat</th>\n",
       "      <th>BurnBndLon</th>\n",
       "      <th>...</th>\n",
       "      <th>Perim_ID</th>\n",
       "      <th>dNBR_offst</th>\n",
       "      <th>dNBR_stdDv</th>\n",
       "      <th>NoData_T</th>\n",
       "      <th>IncGreen_T</th>\n",
       "      <th>Low_T</th>\n",
       "      <th>Mod_T</th>\n",
       "      <th>High_T</th>\n",
       "      <th>Comment</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27219</th>\n",
       "      <td>OR4327211882920180427</td>\n",
       "      <td>75101822-E693-42CB-833B-A79A8C1BBF61</td>\n",
       "      <td>VOLTAGE</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>10014026</td>\n",
       "      <td>MTBS</td>\n",
       "      <td>Initial</td>\n",
       "      <td>1033</td>\n",
       "      <td>43.275</td>\n",
       "      <td>-118.824</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>-970</td>\n",
       "      <td>-150</td>\n",
       "      <td>50</td>\n",
       "      <td>9999</td>\n",
       "      <td>9999</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-1825335.263 2474478.766, -1825325.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27070</th>\n",
       "      <td>OR4441412049420180508</td>\n",
       "      <td>A3A9AE19-4F1A-4550-A924-BF40954361B6</td>\n",
       "      <td>SPEARS RX 0164 PR</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>10013459</td>\n",
       "      <td>MTBS</td>\n",
       "      <td>Initial</td>\n",
       "      <td>1319</td>\n",
       "      <td>44.393</td>\n",
       "      <td>-120.548</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>-11</td>\n",
       "      <td>22</td>\n",
       "      <td>-970</td>\n",
       "      <td>-150</td>\n",
       "      <td>30</td>\n",
       "      <td>9999</td>\n",
       "      <td>9999</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-1929796.319 2630713.538, -1929759.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27064</th>\n",
       "      <td>OR4261612132420180510</td>\n",
       "      <td>None</td>\n",
       "      <td>UNNAMED</td>\n",
       "      <td>Prescribed Fire</td>\n",
       "      <td>10013436</td>\n",
       "      <td>MTBS</td>\n",
       "      <td>Initial</td>\n",
       "      <td>2972</td>\n",
       "      <td>42.616</td>\n",
       "      <td>-121.324</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>48</td>\n",
       "      <td>-970</td>\n",
       "      <td>-150</td>\n",
       "      <td>50</td>\n",
       "      <td>9999</td>\n",
       "      <td>9999</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-2041927.980 2449457.180, -2041907.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26690</th>\n",
       "      <td>OR4261111754220180615</td>\n",
       "      <td>62D347C9-FAA3-410E-8760-C6BD347ACFA4</td>\n",
       "      <td>JACKIES BUTTE</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>10010669</td>\n",
       "      <td>MTBS</td>\n",
       "      <td>Initial</td>\n",
       "      <td>2210</td>\n",
       "      <td>42.595</td>\n",
       "      <td>-117.521</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>-970</td>\n",
       "      <td>-150</td>\n",
       "      <td>110</td>\n",
       "      <td>9999</td>\n",
       "      <td>9999</td>\n",
       "      <td>Burn scar difficult to see. Post fire imagery ...</td>\n",
       "      <td>POLYGON ((-1739322.397 2376359.544, -1739271.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26668</th>\n",
       "      <td>OR4516012107120180621</td>\n",
       "      <td>28B4EDD7-A705-4261-8B45-72DA7DBA5AF6</td>\n",
       "      <td>BOXCAR 0410 RN</td>\n",
       "      <td>Wildfire</td>\n",
       "      <td>10010493</td>\n",
       "      <td>MTBS</td>\n",
       "      <td>Initial</td>\n",
       "      <td>99874</td>\n",
       "      <td>45.022</td>\n",
       "      <td>-121.004</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>-970</td>\n",
       "      <td>-150</td>\n",
       "      <td>95</td>\n",
       "      <td>9999</td>\n",
       "      <td>9999</td>\n",
       "      <td>None</td>\n",
       "      <td>POLYGON ((-1942537.583 2706791.933, -1942582.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Event_ID                               irwinID  \\\n",
       "27219  OR4327211882920180427  75101822-E693-42CB-833B-A79A8C1BBF61   \n",
       "27070  OR4441412049420180508  A3A9AE19-4F1A-4550-A924-BF40954361B6   \n",
       "27064  OR4261612132420180510                                  None   \n",
       "26690  OR4261111754220180615  62D347C9-FAA3-410E-8760-C6BD347ACFA4   \n",
       "26668  OR4516012107120180621  28B4EDD7-A705-4261-8B45-72DA7DBA5AF6   \n",
       "\n",
       "              Incid_Name       Incid_Type    Map_ID Map_Prog Asmnt_Type  \\\n",
       "27219            VOLTAGE         Wildfire  10014026     MTBS    Initial   \n",
       "27070  SPEARS RX 0164 PR  Prescribed Fire  10013459     MTBS    Initial   \n",
       "27064            UNNAMED  Prescribed Fire  10013436     MTBS    Initial   \n",
       "26690      JACKIES BUTTE         Wildfire  10010669     MTBS    Initial   \n",
       "26668     BOXCAR 0410 RN         Wildfire  10010493     MTBS    Initial   \n",
       "\n",
       "       BurnBndAc BurnBndLat BurnBndLon  ... Perim_ID dNBR_offst dNBR_stdDv  \\\n",
       "27219       1033     43.275   -118.824  ...     None          6         16   \n",
       "27070       1319     44.393   -120.548  ...     None        -11         22   \n",
       "27064       2972     42.616   -121.324  ...     None         18         48   \n",
       "26690       2210     42.595   -117.521  ...     None         29         12   \n",
       "26668      99874     45.022   -121.004  ...     None         11         25   \n",
       "\n",
       "      NoData_T  IncGreen_T  Low_T  Mod_T  High_T  \\\n",
       "27219     -970        -150     50   9999    9999   \n",
       "27070     -970        -150     30   9999    9999   \n",
       "27064     -970        -150     50   9999    9999   \n",
       "26690     -970        -150    110   9999    9999   \n",
       "26668     -970        -150     95   9999    9999   \n",
       "\n",
       "                                                 Comment  \\\n",
       "27219                                               None   \n",
       "27070                                               None   \n",
       "27064                                               None   \n",
       "26690  Burn scar difficult to see. Post fire imagery ...   \n",
       "26668                                               None   \n",
       "\n",
       "                                                geometry  \n",
       "27219  POLYGON ((-1825335.263 2474478.766, -1825325.0...  \n",
       "27070  POLYGON ((-1929796.319 2630713.538, -1929759.9...  \n",
       "27064  POLYGON ((-2041927.980 2449457.180, -2041907.6...  \n",
       "26690  POLYGON ((-1739322.397 2376359.544, -1739271.5...  \n",
       "26668  POLYGON ((-1942537.583 2706791.933, -1942582.0...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perims head\n",
    "year_to_perims[2018].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquets in temp2/expanded\n",
    "\n",
    "parq0 = dgpd.read_parquet(\"/home/jake/FireLab/Project/data/temp2/expanded/part.0.parquet\")\n",
    "parq1 = dgpd.read_parquet(\"/home/jake/FireLab/Project/data/temp2/expanded/part.1.parquet\")\n",
    "parq2 = dgpd.read_parquet(\"/home/jake/FireLab/Project/data/temp2/expanded/part.2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask-GeoPandas GeoDataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>geometry</th>\n",
       "      <th>duration</th>\n",
       "      <th>fireid</th>\n",
       "      <th>Ig_Date</th>\n",
       "      <th>buffer</th>\n",
       "      <th>ig_date</th>\n",
       "      <th>state</th>\n",
       "      <th>dem</th>\n",
       "      <th>dem_slope</th>\n",
       "      <th>dem_aspect</th>\n",
       "      <th>dem_flow_acc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>geometry</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>geometry</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>object</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 1 graph layer</div>"
      ],
      "text/plain": [
       "Dask GeoDataFrame Structure:\n",
       "               index  geometry duration  fireid         Ig_Date    buffer         ig_date   state      dem dem_slope dem_aspect dem_flow_acc\n",
       "npartitions=1                                                                                                                               \n",
       "               int64  geometry  float64  object  datetime64[ns]  geometry  datetime64[ns]  object  float32   float32    float32      float32\n",
       "                 ...       ...      ...     ...             ...       ...             ...     ...      ...       ...        ...          ...\n",
       "Dask Name: read-parquet, 1 graph layer"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parq0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dgpd.read_parquet(CHECKPOINT_1_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine df partition columns\n",
    "for i in range(126):\n",
    "    print(\"Partition {} columns: {}\".format(i, df.partitions[i].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State borders\n",
    "print(\"Loading state borders\")\n",
    "stdf = open_vectors(PATHS[\"states\"], 0).data.to_crs(\"EPSG:5071\")\n",
    "states = {st: stdf[stdf.STUSPS == st].geometry for st in list(stdf.STUSPS)}\n",
    "state_shape = states[STATE]\n",
    "states = None\n",
    "stdf = None\n",
    "\n",
    "print(\"Loading VIIRS perimeters\")\n",
    "perimdf = dgpd.read_parquet(DATA_LOC + \"viirs_perims.parquet\").to_crs(\"EPSG:5071\")\n",
    "# print(\"Loading MTBS perimeters\")\n",
    "# perimdf = open_vectors(PATHS[\"mtbs_perim\"]).data.to_crs(\"EPSG:5071\")\n",
    "# rename column t to Ig_Date\n",
    "perimdf = perimdf.rename(columns={\"t\": \"Ig_Date\"})\n",
    "state_fire_perims = perimdf.clip(state_shape.compute())\n",
    "state_fire_perims = (\n",
    "    state_fire_perims.assign(\n",
    "        Ig_Date=lambda frame: dd.to_datetime(\n",
    "            frame.Ig_Date, format=\"%Y-%m-%d\"\n",
    "        )\n",
    "    )\n",
    "    .sort_values(\"Ig_Date\")\n",
    "    .compute()\n",
    ")\n",
    "state_fire_perims = state_fire_perims[state_fire_perims.Ig_Date.dt.year.between(2018, 2020)]\n",
    "year_to_perims = {\n",
    "    y: state_fire_perims[state_fire_perims.Ig_Date.dt.year == y]\n",
    "    for y in YEARS\n",
    "}\n",
    "# year_to_mtbs_file = {\n",
    "#         y: pjoin(PATHS[\"mtbs_root\"], f\"mtbs_{STATE}_{y}.tif\")\n",
    "#         for y in YEARS\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>duration</th>\n",
       "      <th>fireid</th>\n",
       "      <th>Ig_Date</th>\n",
       "      <th>buffer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>POLYGON ((-1944336.649 2546264.822, -1944767.0...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F2518</td>\n",
       "      <td>2020-02-27 12:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>POLYGON ((-1944767.084 2546302.846, -1944785.7...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>F2518</td>\n",
       "      <td>2020-02-28 00:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>POLYGON ((-1944785.743 2546305.433, -1944804.0...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>F2518</td>\n",
       "      <td>2020-03-03 12:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>POLYGON ((-1944912.318 2546394.330, -1944916.7...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>F2518</td>\n",
       "      <td>2020-03-04 00:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>POLYGON ((-1944916.745 2546401.761, -1944916.9...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F2518</td>\n",
       "      <td>2020-03-04 12:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>POLYGON ((-1946999.567 2643406.090, -1946980.5...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>F20175</td>\n",
       "      <td>2020-12-23 12:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>POLYGON ((-1946980.578 2643400.535, -1946962.2...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>F20175</td>\n",
       "      <td>2020-12-24 00:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2325</th>\n",
       "      <td>POLYGON ((-2186703.374 2522749.590, -2186704.1...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F20066</td>\n",
       "      <td>2020-12-24 00:00:00</td>\n",
       "      <td>POLYGON ((-2189271.638 2527956.054, -2189191.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>POLYGON ((-1946114.410 2642787.602, -1946097.0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>F20175</td>\n",
       "      <td>2020-12-24 12:00:00</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2328</th>\n",
       "      <td>POLYGON ((-1946097.034 2642781.286, -1946096.8...</td>\n",
       "      <td>7.5</td>\n",
       "      <td>F20175</td>\n",
       "      <td>2020-12-29 00:00:00</td>\n",
       "      <td>POLYGON ((-1956789.836 2641154.876, -1956871.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>683 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               geometry  duration  fireid  \\\n",
       "1505  POLYGON ((-1944336.649 2546264.822, -1944767.0...       0.0   F2518   \n",
       "1506  POLYGON ((-1944767.084 2546302.846, -1944785.7...       0.5   F2518   \n",
       "1507  POLYGON ((-1944785.743 2546305.433, -1944804.0...       5.0   F2518   \n",
       "1508  POLYGON ((-1944912.318 2546394.330, -1944916.7...       5.5   F2518   \n",
       "1509  POLYGON ((-1944916.745 2546401.761, -1944916.9...       6.0   F2518   \n",
       "...                                                 ...       ...     ...   \n",
       "2324  POLYGON ((-1946999.567 2643406.090, -1946980.5...       2.0  F20175   \n",
       "2326  POLYGON ((-1946980.578 2643400.535, -1946962.2...       2.5  F20175   \n",
       "2325  POLYGON ((-2186703.374 2522749.590, -2186704.1...       6.0  F20066   \n",
       "2327  POLYGON ((-1946114.410 2642787.602, -1946097.0...       3.0  F20175   \n",
       "2328  POLYGON ((-1946097.034 2642781.286, -1946096.8...       7.5  F20175   \n",
       "\n",
       "                 Ig_Date                                             buffer  \n",
       "1505 2020-02-27 12:00:00                                               None  \n",
       "1506 2020-02-28 00:00:00                                               None  \n",
       "1507 2020-03-03 12:00:00                                               None  \n",
       "1508 2020-03-04 00:00:00                                               None  \n",
       "1509 2020-03-04 12:00:00                                               None  \n",
       "...                  ...                                                ...  \n",
       "2324 2020-12-23 12:00:00                                               None  \n",
       "2326 2020-12-24 00:00:00                                               None  \n",
       "2325 2020-12-24 00:00:00  POLYGON ((-2189271.638 2527956.054, -2189191.4...  \n",
       "2327 2020-12-24 12:00:00                                               None  \n",
       "2328 2020-12-29 00:00:00  POLYGON ((-1956789.836 2641154.876, -1956871.5...  \n",
       "\n",
       "[683 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_to_perims[2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_obj = Raster(raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building mtbs df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MTBS: 100%|███████████████████████████████████████| 3/3 [00:08<00:00,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########                                ] | 20% Completed | 117.08 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 610.69 ms\n"
     ]
    }
   ],
   "source": [
    "df = build_mtbs_df(\n",
    "            YEARS,\n",
    "            year_to_perims=year_to_perims,\n",
    "            state=STATE,\n",
    "            out_path=MTBS_DF_TEMP_PATH_2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51997410"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_mtbs_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_shape.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new GeoDataFrame with full geometry of Oregon and entries for every week in the range 2018-2020\n",
    "oregon_gdf = gpd.GeoDataFrame(geometry=state_shape.geometry.compute())\n",
    "oregon_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range\n",
    "date_range = pd.date_range(start='2018-01-01', end='2020-12-31', freq='W')\n",
    "\n",
    "# Calculate the number of dummy rows needed\n",
    "num_dummy_rows = len(date_range) - len(oregon_gdf)\n",
    "\n",
    "# Create a DataFrame with the dummy rows\n",
    "dummy_df = pd.DataFrame(index=range(num_dummy_rows))\n",
    "\n",
    "# Concatenate the original DataFrame with the dummy DataFrame\n",
    "oregon_gdf = pd.concat([oregon_gdf, dummy_df])\n",
    "\n",
    "# Assign the date range to the new column 'Ig_Date'\n",
    "oregon_gdf['Ig_Date'] = date_range\n",
    "oregon_gdf\n",
    "oregon_gdf.reset_index(drop=True, inplace=True)\n",
    "oregon_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(oregon_gdf)):\n",
    "    oregon_gdf.geometry[i] = oregon_gdf.geometry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oregon_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_mtbs_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_perims = dgpd.read_parquet(DATA_LOC + \"viirs_perims.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    viirs_perims_computed = viirs_perims.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_perims_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe where geometry polygons are converted to points within the polygon at a 30m resolution\n",
    "viirs_perims_points = viirs_perims_computed.to_crs(\"EPSG:5071\").explode().reset_index(drop=True)\n",
    "viirs_perims_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viirs_perims.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Assuming viirs_perims_computed is a GeoDataFrame\n",
    "fireid = \"F4734\"\n",
    "fire = viirs_perims_computed[viirs_perims_computed.fireid == fireid]\n",
    "\n",
    "# Prepare figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Determine the bounds from the final perimeter\n",
    "final_perimeter = fire[fire.duration == fire.duration.max()].geometry\n",
    "minx, miny, maxx, maxy = final_perimeter.total_bounds\n",
    "\n",
    "def animate(i):\n",
    "    ax.clear()  # Clear the plot to draw a new frame\n",
    "    # Plot the fire perimeter at duration 'i'\n",
    "    fire[fire.duration == i].geometry.plot(ax=ax)\n",
    "    ax.set_title(f\"Fire duration step: {i}\")\n",
    "    # Set the same bounds for each frame\n",
    "    ax.set_xlim(minx, maxx)\n",
    "    ax.set_ylim(miny, maxy)\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, animate, frames=fire.duration.unique(), interval=500)\n",
    "\n",
    "# To save the animation as a GIF:\n",
    "ani.save(f\"{fireid}.gif\", writer=\"imagemagick\", fps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from parquet new\n",
    "with ProgressBar():\n",
    "    df = dgpd.read_parquet(MTBS_DF_PARQUET_PATH_NEW)#.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df metadata\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"ig_date\"] == \"2016-04-15\"].plot(column=\"dem\", cmap=\"terrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load perimeters\n",
    "perimdf = open_vectors(PATHS[\"mtbs_perim\"]).data.to_crs(\"EPSG:5071\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
