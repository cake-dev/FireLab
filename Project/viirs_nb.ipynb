{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import warnings\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask_geopandas as dgpd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from scipy.fft import dst\n",
    "import tqdm\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "from raster_tools import Raster, Vector, open_vectors, clipping, zonal\n",
    "from raster_tools.dtypes import F32, U8, U16, F16\n",
    "\n",
    "# Filter out warnings from dask_geopandas and dask\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*initial implementation of Parquet.*\"\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", message=\".*Slicing is producing a large chunk.*\"\n",
    ")\n",
    "\n",
    "\n",
    "# Location for temporary storage\n",
    "TMP_LOC = \"/home/jake/FireLab/Project/data/temp/\"\n",
    "TMP_LOC2 = \"/home/jake/FireLab/Project/data/temp2/\"\n",
    "TMP_LOC3 = \"/home/jake/FireLab/Project/data/temp3/\"\n",
    "DATA_LOC = \"/home/jake/FireLab/Project/data/\"\n",
    "\n",
    "STATE = \"OR\"\n",
    "\n",
    "# Location of clipped DEM files\n",
    "DEM_DATA_DIR = pjoin(TMP_LOC, \"dem_data\")\n",
    "\n",
    "# location of feature data files\n",
    "FEATURE_DIR = pjoin(DATA_LOC, \"FeatureData\")\n",
    "EDNA_DIR = pjoin(DATA_LOC, \"terrain\")\n",
    "MTBS_DIR = pjoin(DATA_LOC, \"MTBS_Data\")\n",
    "VIIRS_DIR = pjoin(DATA_LOC, \"viirs_data\")\n",
    "\n",
    "PATHS = {\n",
    "    \"states\": pjoin(EDNA_DIR, \"state_borders/cb_2018_us_state_5m.shp\"),\n",
    "    \"dem\": pjoin(EDNA_DIR, \"us_orig_dem/us_orig_dem/orig_dem/hdr.adf\"),\n",
    "    \"dem_slope\": pjoin(EDNA_DIR, \"us_slope/us_slope/slope/hdr.adf\"),\n",
    "    \"dem_aspect\": pjoin(EDNA_DIR, \"us_aspect/aspect/hdr.adf\"),\n",
    "    \"dem_flow_acc\": pjoin(EDNA_DIR, \"us_flow_acc/us_flow_acc/flow_acc/hdr.adf\"),\n",
    "    \"gm_srad\": pjoin(FEATURE_DIR, \"gridmet/srad_1986_2020_weekly.nc\"),\n",
    "    \"gm_vpd\": pjoin(FEATURE_DIR, \"gridmet/vpd_1986_2020_weekly.nc\"),\n",
    "    \"aw_mat\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MAT.tif\"),\n",
    "    \"aw_mcmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MCMT.tif\"),\n",
    "    \"aw_mwmt\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_MWMT.tif\"),\n",
    "    \"aw_td\": pjoin(FEATURE_DIR, \"adaptwest/Normal_1991_2020_TD.tif\"),\n",
    "    \"dm_tmax\": pjoin(FEATURE_DIR, \"daymet/tmax_1986_2020.nc\"),\n",
    "    \"dm_tmin\": pjoin(FEATURE_DIR, \"daymet/tmin_1986_2020.nc\"),\n",
    "    \"biomass_afg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_afg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"biomass_pfg\": pjoin(\n",
    "        FEATURE_DIR, \"biomass/biomass_pfg_1986_2020_{}.nc\".format(STATE)\n",
    "    ),\n",
    "    \"landfire_fvt\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FVT_200_CONUS/Tif/LC20_FVT_200.tif\"\n",
    "    ),\n",
    "    \"landfire_fbfm40\": pjoin(\n",
    "        FEATURE_DIR, \"landfire/LF2020_FBFM40_200_CONUS/Tif/LC20_F40_200.tif\"\n",
    "    ),\n",
    "    \"ndvi\": pjoin(FEATURE_DIR, \"ndvi/access/weekly/ndvi_1986_2020_weekavg.nc\"),\n",
    "    \"mtbs_root\": pjoin(MTBS_DIR, \"MTBS_BSmosaics/\"),\n",
    "    \"mtbs_perim\": pjoin(MTBS_DIR, \"mtbs_perimeter_data/mtbs_perims_DD.shp\"),\n",
    "    \"viirs_root\": VIIRS_DIR,\n",
    "    \"viirs_perim\": pjoin(VIIRS_DIR, \"viirs_perims_shapefile.shp\"),\n",
    "}\n",
    "YEARS = list(range(2018, 2021))\n",
    "GM_KEYS = list(filter(lambda x: x.startswith(\"gm_\"), PATHS))\n",
    "AW_KEYS = list(filter(lambda x: x.startswith(\"aw_\"), PATHS))\n",
    "DM_KEYS = list(filter(lambda x: x.startswith(\"dm_\"), PATHS))\n",
    "BIOMASS_KEYS = list(filter(lambda x: x.startswith(\"biomass_\"), PATHS))\n",
    "LANDFIRE_KEYS = list(filter(lambda x: x.startswith(\"landfire_\"), PATHS))\n",
    "NDVI_KEYS = list(filter(lambda x: x.startswith(\"ndvi\"), PATHS))\n",
    "DEM_KEYS = list(filter(lambda x: x.startswith(\"dem\"), PATHS))\n",
    "\n",
    "# NC_KEYSET = set(GM_KEYS + DM_KEYS + BIOMASS_KEYS + NDVI_KEYS)\n",
    "NC_KEYSET = [DM_KEYS, GM_KEYS, BIOMASS_KEYS, NDVI_KEYS]\n",
    "TIF_KEYSET = [AW_KEYS, LANDFIRE_KEYS]\n",
    "\n",
    "MTBS_DF_PATH = pjoin(TMP_LOC, f\"{STATE}_mtbs.parquet\")\n",
    "MTBS_DF_PARQUET_PATH_NEW = pjoin(TMP_LOC, f\"{STATE}_mtbs_new.parquet\")\n",
    "MTBS_DF_TEMP_PATH = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp.parquet\")\n",
    "MTBS_DF_TEMP_PATH_2 = pjoin(TMP_LOC, f\"{STATE}_mtbs_temp_2.parquet\")\n",
    "CHECKPOINT_1_PATH = pjoin(TMP_LOC, \"check1\")\n",
    "CHECKPOINT_2_PATH = pjoin(TMP_LOC, \"check2\")\n",
    "CHECKPOINT_3_PATH = pjoin(TMP_LOC, \"check3\")\n",
    "CHECKPOINT_4_PATH = pjoin(TMP_LOC, \"check4\")\n",
    "\n",
    "\n",
    "def hillshade(slope, aspect, azimuth=180, zenith=45):\n",
    "    # Convert angles from degrees to radians\n",
    "    azimuth_rad = np.radians(azimuth)\n",
    "    zenith_rad = np.radians(zenith)\n",
    "    slope_rad = np.radians(slope)\n",
    "    aspect_rad = np.radians(aspect)\n",
    "\n",
    "    # Calculate hillshade\n",
    "    shaded = np.sin(zenith_rad) * np.sin(slope_rad) + \\\n",
    "             np.cos(zenith_rad) * np.cos(slope_rad) * \\\n",
    "             np.cos(azimuth_rad - aspect_rad)\n",
    "    # scale to 0-255\n",
    "    shaded = 255 * (shaded + 1) / 2\n",
    "    # round hillshade to nearest integer\n",
    "    shaded = np.rint(shaded)\n",
    "    # convert to uint8\n",
    "    # Ensure non-finite values are not converted to int\n",
    "    # shaded = np.where(np.isfinite(shaded), shaded.astype(np.uint8), np.nan)\n",
    "    return shaded\n",
    "\n",
    "def hillshade_partition(df, zenith, azimuth):\n",
    "    # Apply the hillshade function to the slope and aspect columns\n",
    "    df['hillshade'] = hillshade(df['dem_slope'], df['dem_aspect'], azimuth, zenith)\n",
    "    return df\n",
    "\n",
    "def timestamp_to_year_part(df):\n",
    "    # Assuming 'ig_date' is the column with timestamp data\n",
    "    df['year'] = df['ig_date'].dt.year\n",
    "    return df\n",
    "\n",
    "def get_nc_var_name(ds, nc_feat_name):\n",
    "    # Find the data variable in a nc xarray.Dataset\n",
    "    if nc_feat_name.startswith(\"dm\"):\n",
    "        var_name = list(set(ds.keys()) - set([\"crs\", \"bnds\"]))[0] # for DAYMET ONLY!!\n",
    "    else:\n",
    "        var_name = list(set(ds.keys()) - set([\"crs\", \"day_bnds\"]))[0]\n",
    "    return var_name\n",
    "\n",
    "\n",
    "def netcdf_to_raster(path, date, nc_feature_name):\n",
    "    # This produces a Dataset. We need to grab the DataArray inside that\n",
    "    # contains the data of interest.\n",
    "    nc_ds = xr.open_dataset(path, chunks={\"day\": 1})#, decode_times=False)\n",
    "    if nc_feature_name == \"ndvi\":\n",
    "        nc_ds2 = nc_ds.drop_vars(\n",
    "        [\"latitude_bnds\", \"longitude_bnds\", \"time_bnds\"]\n",
    "        ).rio.write_crs(\"EPSG:5071\") # FOR NDVI ONLY!!\n",
    "    elif nc_feature_name.startswith(\"dm_\"):\n",
    "        nc_ds2 = nc_ds.rio.write_crs(\"EPSG:5071\")  # FOR DAYMET ONLY!!\n",
    "        # nc_ds = nc_ds.rio.write_crs(\n",
    "        #     nc_ds.coords[\"lambert_conformal_conic\"].spatial_ref\n",
    "        # )  # FOR DAYMET ONLY!!\n",
    "        nc_ds2 = nc_ds2.rename({\"lambert_conformal_conic\": \"crs\"})  # FOR DAYMET ONLY!!\n",
    "        nc_ds2 = nc_ds2.drop_vars([\"lat\", \"lon\", \"time_bnds\"])  # FOR DAYMET ONLY!!\n",
    "        nc_ds = None # FOR DAYMET ONLY!!\n",
    "        nc_ds2 = nc_ds2.rename_vars({\"x\": \"lon\", \"y\": \"lat\"})  # FOR DAYMET ONLY!!\n",
    "    else:\n",
    "        nc_ds2 = nc_ds.rio.write_crs(\"EPSG:5071\")\n",
    "\n",
    "    # Find variable name\n",
    "    var_name = get_nc_var_name(nc_ds2, nc_feature_name)\n",
    "    # print(f\"var_name: {var_name}\")\n",
    "    # Extract\n",
    "    var_da = nc_ds2[var_name]\n",
    "    # print(f\"{var_da = }\")\n",
    "    if nc_feature_name.startswith(\"gm_\"):\n",
    "        var_da = var_da.sel(day=date, method=\"nearest\") # for GM\n",
    "    else:\n",
    "        var_da = var_da.sel(time=date, method=\"nearest\") # for DM and BM and NDVI\n",
    "    if nc_feature_name.startswith(\"ndvi\"):\n",
    "        xrs = xr.DataArray(\n",
    "            var_da.data, dims=(\"y\", \"x\"), coords=(var_da.latitude.data, var_da.longitude.data)\n",
    "        ).expand_dims(\"band\") # FOR NDVI ONLY!!\n",
    "    else:\n",
    "        xrs = xr.DataArray(\n",
    "            var_da.data, dims=(\"y\", \"x\"), coords=(var_da.lat.data, var_da.lon.data)\n",
    "        ).expand_dims(\"band\") # For non-NDVI\n",
    "    xrs[\"band\"] = [1]\n",
    "    # Set CRS in raster compliant format\n",
    "    xrs = xrs.rio.write_crs(nc_ds2.crs.spatial_ref)\n",
    "    return Raster(xrs)\n",
    "\n",
    "\n",
    "def extract_nc_data(df, nc_name):\n",
    "    assert df.ig_date.unique().size == 1\n",
    "    # print(f\"{gm_name}: {df.columns = }, {len(df) = }\")\n",
    "    date = df.ig_date.values[0]\n",
    "    print(f\"{nc_name}: starting {date}\")\n",
    "    rs = netcdf_to_raster(PATHS[nc_name], date, nc_name)\n",
    "    bounds = gpd.GeoSeries(df.geometry).to_crs(rs.crs).total_bounds\n",
    "    rs = clipping.clip_box(rs, bounds)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .rename(columns={\"extracted\": nc_name})\n",
    "        .compute()\n",
    "    )\n",
    "    df[nc_name].values[:] = rdf[nc_name].values\n",
    "    # print(f\"{nc_name}: finished {date}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_state_dem_path(dem_key, state):\n",
    "    return pjoin(DEM_DATA_DIR, f\"{state}_{dem_key}.tif\")\n",
    "\n",
    "\n",
    "def extract_dem_data(df, key):\n",
    "    state = df.state.values[0]\n",
    "    path = get_state_dem_path(key, state)\n",
    "    rs = Raster(path)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .compute()\n",
    "    )\n",
    "    df[key].values[:] = rdf.extracted.values\n",
    "    return df\n",
    "\n",
    "def extract_tif_data(df, key):\n",
    "    state = df.state.values[0]\n",
    "    path = PATHS[key]\n",
    "    rs = Raster(path)\n",
    "    if type(df) == pd.DataFrame:\n",
    "        df = gpd.GeoDataFrame(df)\n",
    "    feat = Vector(df, len(df))\n",
    "    rdf = (\n",
    "        zonal.extract_points_eager(feat, rs, skip_validation=True)\n",
    "        .drop(columns=[\"band\"])\n",
    "        .compute()\n",
    "    )\n",
    "    df[key].values[:] = rdf.extracted.values\n",
    "    return df\n",
    "\n",
    "\n",
    "def partition_extract_nc(df, key):\n",
    "    # This func wraps extract_nc_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_nc_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_nc_data(gdf, key))\n",
    "    return pd.concat(parts)\n",
    "\n",
    "def partition_extract_tif(df, key):\n",
    "    # This func wraps extract_tif_data. It groups the partition in to sub\n",
    "    # dataframes with the same date and then applies extract_tif_data to\n",
    "    # each and reassembles the results into an output dataframe.\n",
    "    parts = []\n",
    "    for group in df.groupby(\"ig_date\", sort=True):\n",
    "        _, gdf = group\n",
    "        parts.append(extract_tif_data(gdf, key))\n",
    "    return pd.concat(parts)\n",
    "\n",
    "def clip_and_save_dem_rasters(keys, paths, feature, state):\n",
    "    feature = feature.compute()\n",
    "    for k in tqdm.tqdm(keys, ncols=80, desc=\"DEM Clipping\"):\n",
    "        path = paths[k]\n",
    "        out_path = get_state_dem_path(k, state)\n",
    "        if os.path.exists(out_path):\n",
    "            continue\n",
    "        rs = Raster(path)\n",
    "        (bounds,) = dask.compute(feature.to_crs(rs.crs).total_bounds)\n",
    "        crs = clipping.clip_box(rs, bounds)\n",
    "        crs.save(out_path)\n",
    "\n",
    "def build_mtbs_year_df(perims_df, state_label):\n",
    "    dfs = []\n",
    "    # print(\"perims df at build_mtbs_year_df: \", perims_df.head())\n",
    "    for grp in perims_df.groupby([\"Ig_Date\", \"duration\"]):\n",
    "        dates, perim = grp\n",
    "        date = dates[0]\n",
    "        duration = dates[1]\n",
    "        perim[\"ig_date\"] = date\n",
    "        perim[\"state\"] = state_label\n",
    "        perim[\"duration\"] = duration\n",
    "        df = perim.assign(\n",
    "            ig_date=lambda frame: dd.to_datetime(\n",
    "                frame.ig_date, format=\"%Y-%m-%d\"\n",
    "            ),\n",
    "            state=lambda frame: frame.state.astype(\"category\"),\n",
    "            duration=lambda frame: frame.duration.astype(\"float32\"),\n",
    "            fireid=lambda frame: frame.fireid\n",
    "        )\n",
    "        # df.drop(columns=[\"Ig_Date\", \"duration\", \"fireid\"], inplace=True)\n",
    "        dfs.append(df)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "\n",
    "def _build_mtbs_df(\n",
    "    years, year_to_perims, state, working_dir\n",
    "):\n",
    "    dfs = []\n",
    "    it = tqdm.tqdm(years, ncols=80, desc=\"MTBS\")\n",
    "    for y in it:\n",
    "        # mtbs_path = year_to_mtbs_file[y]\n",
    "        # if not os.path.exists(mtbs_path):\n",
    "        #     it.write(f\"No data for {y}\")\n",
    "        #     continue\n",
    "        perims = year_to_perims[y]\n",
    "        ydf = build_mtbs_year_df(perims, state)\n",
    "        ypath = pjoin(working_dir, str(y))\n",
    "        ydf.compute().to_parquet(ypath)\n",
    "        ydf = dgpd.read_parquet(ypath)\n",
    "        dfs.append(ydf)\n",
    "    return dd.concat(dfs)\n",
    "\n",
    "\n",
    "def build_mtbs_df(\n",
    "    years, year_to_perims, state, out_path, tmp_loc=TMP_LOC2\n",
    "):\n",
    "    print(\"Building mtbs df\")\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        df = _build_mtbs_df(\n",
    "            years, year_to_perims, state, working_dir\n",
    "        )\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)\n",
    "\n",
    "\n",
    "def add_columns_to_df(\n",
    "    df,\n",
    "    columns,\n",
    "    part_func,\n",
    "    out_path,\n",
    "    col_type=F32,\n",
    "    col_default=np.nan,\n",
    "    part_func_args=(),\n",
    "    # tmp_loc=TMP_LOC,\n",
    "    tmp_loc=TMP_LOC3,\n",
    "    parallel=True,\n",
    "):\n",
    "    print(f\"Adding columns: {columns}\")\n",
    "    print('Existing columns: ', df.columns)\n",
    "    # Add columns\n",
    "    expanded_df = df.assign(**{c: col_type.type(col_default) for c in columns})\n",
    "    # show expanded_df\n",
    "    print('Expanded columns: ', expanded_df.columns)\n",
    "    with tempfile.TemporaryDirectory(dir=tmp_loc) as working_dir:\n",
    "        # Save to disk before applying partition function. to_parquet() has a\n",
    "        # chance of segfaulting and that chance goes WAY up after adding\n",
    "        # columns and then mapping a function to partitions. Saving to disk\n",
    "        # before mapping keeps the odds low.\n",
    "        # working_dir = tmp_loc\n",
    "        path = pjoin(working_dir, \"expanded\")\n",
    "        # show the type of expanded_df\n",
    "        print('expanded_df type: ', type(expanded_df))\n",
    "        print(expanded_df.head())\n",
    "        # reset the index to fix keyerror not in index\n",
    "        expanded_df = expanded_df.reset_index()\n",
    "        expanded_df.to_parquet(path)\n",
    "        expanded_df = dgpd.read_parquet(path)\n",
    "        meta = expanded_df._meta.copy()\n",
    "        for c in columns:\n",
    "            expanded_df = expanded_df.map_partitions(\n",
    "                part_func, c, *part_func_args, meta=meta\n",
    "            )\n",
    "        if parallel:\n",
    "            with ProgressBar():\n",
    "                expanded_df.to_parquet(out_path)\n",
    "        else:\n",
    "            # Save parts in serial and then assemble into single dataframe\n",
    "            with tempfile.TemporaryDirectory(dir=tmp_loc) as part_dir:\n",
    "                dfs = []\n",
    "                for i, part in enumerate(expanded_df.partitions):\n",
    "                    # Save part i\n",
    "                    part_path = pjoin(part_dir, f\"part{i:04}\")\n",
    "                    with ProgressBar():\n",
    "                        part.compute().to_parquet(part_path)\n",
    "                    # Save paths for opening with dask_geopandas later. Avoid\n",
    "                    # opening more dataframes in this loop as doing so will\n",
    "                    # likely cause a segfault. I have no idea why.\n",
    "                    dfs.append(part_path)\n",
    "                dfs = [dgpd.read_parquet(p) for p in dfs]\n",
    "                # Assemble and save to final output location\n",
    "                expanded_df = dd.concat(dfs)\n",
    "                with ProgressBar():\n",
    "                    expanded_df.to_parquet(out_path)\n",
    "    return dgpd.read_parquet(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading state borders\n",
      "Loading VIIRS perimeters\n",
      "Time elapsed: 7.373118877410889 seconds\n"
     ]
    }
   ],
   "source": [
    "### MAIN ###\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "\n",
    "    if 1:\n",
    "        # State borders\n",
    "        print(\"Loading state borders\")\n",
    "        stdf = open_vectors(PATHS[\"states\"], 0).data.to_crs(\"EPSG:5071\")\n",
    "        states = {st: stdf[stdf.STUSPS == st].geometry for st in list(stdf.STUSPS)}\n",
    "        state_shape = states[STATE]\n",
    "        states = None\n",
    "        stdf = None\n",
    "\n",
    "        print(\"Loading VIIRS perimeters\")\n",
    "        # perimdf = gpd.read_parquet(PATHS[\"viirs_perim\"]).to_crs(\"EPSG:5071\")\n",
    "        perimdf = open_vectors(PATHS[\"viirs_perim\"]).data.to_crs(\"EPSG:5071\")\n",
    "        # rename column t to Ig_Date\n",
    "        # perimdf = perimdf.rename(columns={\"t\": \"Ig_Date\"})\n",
    "        state_fire_perims = perimdf.clip(state_shape.compute())\n",
    "        state_fire_perims = (\n",
    "            state_fire_perims.assign(\n",
    "                Ig_Date=lambda frame: dd.to_datetime(\n",
    "                    frame.Ig_Date, format=\"%Y-%m-%d\"\n",
    "                )\n",
    "            )\n",
    "            .sort_values(\"Ig_Date\")\n",
    "            .compute()\n",
    "        )\n",
    "        state_fire_perims = state_fire_perims[state_fire_perims.Ig_Date.dt.year.between(2018, 2020)]\n",
    "        year_to_perims = {\n",
    "            y: state_fire_perims[state_fire_perims.Ig_Date.dt.year == y]\n",
    "            for y in YEARS\n",
    "        }\n",
    "\n",
    "        year_to_mtbs_file = {\n",
    "            y: pjoin(PATHS[\"mtbs_root\"], f\"mtbs_{STATE}_{y}.tif\")\n",
    "            for y in YEARS\n",
    "        }\n",
    "\n",
    "        # print(year_to_mtbs_file)\n",
    "\n",
    "\n",
    "    if 0:\n",
    "        # code below for creating a new dataset for a new state / region\n",
    "        df = build_mtbs_df(\n",
    "            YEARS,\n",
    "            year_to_perims=year_to_perims,\n",
    "            state=STATE,\n",
    "            out_path=MTBS_DF_TEMP_PATH_2,\n",
    "        )\n",
    "        clip_and_save_dem_rasters(DEM_KEYS, PATHS, state_shape, STATE)\n",
    "        df = add_columns_to_df(\n",
    "            df,\n",
    "            DEM_KEYS,\n",
    "            extract_dem_data,\n",
    "            CHECKPOINT_1_PATH,\n",
    "            # Save results in serial to avoid segfaulting. Something about the\n",
    "            # dem computations makes segfaults extremely likely when saving\n",
    "            # The computations require a lot of memory which may be what\n",
    "            # triggers the fault.\n",
    "            parallel=False,\n",
    "        )\n",
    "        df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "        print(\"Repartitioning\")\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(CHECKPOINT_2_PATH)\n",
    "        df = None\n",
    "\n",
    "    if 0:\n",
    "        # code below used to add new features to the dataset\n",
    "        with ProgressBar():\n",
    "            df = dgpd.read_parquet(CHECKPOINT_2_PATH)\n",
    "\n",
    "        # loop to add all nc features\n",
    "        for nc_name in NC_KEYSET:\n",
    "            print(f\"Adding {nc_name}\")\n",
    "            df = add_columns_to_df(\n",
    "                df, nc_name, partition_extract_nc, CHECKPOINT_1_PATH, parallel=False\n",
    "            )\n",
    "        # loop to add all tif features\n",
    "        # for tif_name in TIF_KEYSET:\n",
    "        #     print(f\"Adding {tif_name}\")\n",
    "        #     df = add_columns_to_df(\n",
    "        #         df, [tif_name], partition_extract_tif, CHECKPOINT_1_PATH, parallel=False\n",
    "        #     )\n",
    "        # add hillshade and year columns\n",
    "        df_meta = df._meta.copy()\n",
    "        df = df.assign(hillshade=U8.type(0))\n",
    "        df = df.map_partitions(hillshade_partition, 45, 180, meta=df_meta)\n",
    "        df = df.assign(year=U16.type(0))\n",
    "        df = df.map_partitions(timestamp_to_year_part, meta=df_meta)\n",
    "        print(\"Columns: \", df.columns)\n",
    "        df = df.repartition(partition_size=\"100MB\").reset_index(drop=True)\n",
    "        print(\"Repartitioning\")\n",
    "        df = df.compute()\n",
    "        with ProgressBar():\n",
    "            df.to_parquet(MTBS_DF_TEMP_PATH)\n",
    "\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "    print(f\"Time elapsed: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = pjoin(VIIRS_DIR, \"viirs_raster.tif\")\n",
    "rs = Raster(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>fireid</th>\n",
       "      <th>Ig_Date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>POLYGON ((-2040844.099 2451920.709, -2040826.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>F4693</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>POLYGON ((-2009343.166 2399585.893, -2009341.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>F4693</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>POLYGON ((-2009067.097 2399902.223, -2009053.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>POLYGON ((-2041844.911 2451280.637, -2041844.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>POLYGON ((-2041844.983 2451282.932, -2041845.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration fireid     Ig_Date  \\\n",
       "0       0.0  F4734  2018-04-24   \n",
       "1       0.0  F4693  2018-04-24   \n",
       "2       1.0  F4693  2018-04-25   \n",
       "3       1.0  F4734  2018-04-25   \n",
       "4       1.5  F4734  2018-04-26   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-2040844.099 2451920.709, -2040826.0...  \n",
       "1  POLYGON ((-2009343.166 2399585.893, -2009341.1...  \n",
       "2  POLYGON ((-2009067.097 2399902.223, -2009053.3...  \n",
       "3  POLYGON ((-2041844.911 2451280.637, -2041844.9...  \n",
       "4  POLYGON ((-2041844.983 2451282.932, -2041845.2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perimdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>fireid</th>\n",
       "      <th>Ig_Date</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-24</td>\n",
       "      <td>POLYGON ((-2040844.099 2451920.709, -2040826.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-25</td>\n",
       "      <td>POLYGON ((-2041844.911 2451280.637, -2041844.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>POLYGON ((-2041844.983 2451282.932, -2041845.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-26</td>\n",
       "      <td>POLYGON ((-2041845.246 2451285.527, -2041845.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-04-27</td>\n",
       "      <td>POLYGON ((-2041710.810 2451919.399, -2041704.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>POLYGON ((-2041704.339 2451936.375, -2041696.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-03</td>\n",
       "      <td>POLYGON ((-2040036.171 2450723.203, -2040036.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>POLYGON ((-2040036.233 2450723.175, -2040053.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-04</td>\n",
       "      <td>POLYGON ((-2040053.491 2450717.294, -2040687.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-05</td>\n",
       "      <td>POLYGON ((-2040687.753 2450534.539, -2040691.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-06</td>\n",
       "      <td>POLYGON ((-2040691.632 2450533.465, -2040698.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-07</td>\n",
       "      <td>POLYGON ((-2040698.861 2450531.547, -2041335.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>POLYGON ((-2041335.202 2450023.855, -2041511.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>POLYGON ((-2041511.005 2449348.186, -2041531.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>16.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>POLYGON ((-2041531.530 2448991.898, -2041533.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>16.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-11</td>\n",
       "      <td>POLYGON ((-2041533.535 2448973.364, -2041537.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-12</td>\n",
       "      <td>POLYGON ((-2041537.389 2448955.108, -2041543.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-12</td>\n",
       "      <td>POLYGON ((-2041543.054 2448937.313, -2041550.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-13</td>\n",
       "      <td>POLYGON ((-2041550.474 2448920.155, -2041559.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.5</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>POLYGON ((-2041559.574 2448903.805, -2041570.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20.0</td>\n",
       "      <td>F4734</td>\n",
       "      <td>2018-05-14</td>\n",
       "      <td>POLYGON ((-2041570.265 2448888.425, -2041582.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    duration fireid     Ig_Date  \\\n",
       "0        0.0  F4734  2018-04-24   \n",
       "3        1.0  F4734  2018-04-25   \n",
       "4        1.5  F4734  2018-04-26   \n",
       "7        2.0  F4734  2018-04-26   \n",
       "8        3.0  F4734  2018-04-27   \n",
       "10       8.0  F4734  2018-05-02   \n",
       "11       9.0  F4734  2018-05-03   \n",
       "12       9.5  F4734  2018-05-04   \n",
       "13      10.0  F4734  2018-05-04   \n",
       "14      11.0  F4734  2018-05-05   \n",
       "15      11.5  F4734  2018-05-06   \n",
       "16      13.0  F4734  2018-05-07   \n",
       "18      13.5  F4734  2018-05-08   \n",
       "20      14.0  F4734  2018-05-08   \n",
       "22      16.0  F4734  2018-05-10   \n",
       "23      16.5  F4734  2018-05-11   \n",
       "24      17.5  F4734  2018-05-12   \n",
       "25      18.0  F4734  2018-05-12   \n",
       "26      19.0  F4734  2018-05-13   \n",
       "27      19.5  F4734  2018-05-14   \n",
       "29      20.0  F4734  2018-05-14   \n",
       "\n",
       "                                             geometry  \n",
       "0   POLYGON ((-2040844.099 2451920.709, -2040826.0...  \n",
       "3   POLYGON ((-2041844.911 2451280.637, -2041844.9...  \n",
       "4   POLYGON ((-2041844.983 2451282.932, -2041845.2...  \n",
       "7   POLYGON ((-2041845.246 2451285.527, -2041845.3...  \n",
       "8   POLYGON ((-2041710.810 2451919.399, -2041704.3...  \n",
       "10  POLYGON ((-2041704.339 2451936.375, -2041696.2...  \n",
       "11  POLYGON ((-2040036.171 2450723.203, -2040036.2...  \n",
       "12  POLYGON ((-2040036.233 2450723.175, -2040053.4...  \n",
       "13  POLYGON ((-2040053.491 2450717.294, -2040687.7...  \n",
       "14  POLYGON ((-2040687.753 2450534.539, -2040691.6...  \n",
       "15  POLYGON ((-2040691.632 2450533.465, -2040698.8...  \n",
       "16  POLYGON ((-2040698.861 2450531.547, -2041335.2...  \n",
       "18  POLYGON ((-2041335.202 2450023.855, -2041511.0...  \n",
       "20  POLYGON ((-2041511.005 2449348.186, -2041531.5...  \n",
       "22  POLYGON ((-2041531.530 2448991.898, -2041533.5...  \n",
       "23  POLYGON ((-2041533.535 2448973.364, -2041537.3...  \n",
       "24  POLYGON ((-2041537.389 2448955.108, -2041543.0...  \n",
       "25  POLYGON ((-2041543.054 2448937.313, -2041550.4...  \n",
       "26  POLYGON ((-2041550.474 2448920.155, -2041559.5...  \n",
       "27  POLYGON ((-2041559.574 2448903.805, -2041570.2...  \n",
       "29  POLYGON ((-2041570.265 2448888.425, -2041582.4...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perimdf[perimdf.fireid== 'F4734']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf = perimdf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas max rows\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireid\n",
       "F8994     151\n",
       "F8954     145\n",
       "F9444     140\n",
       "F9034     115\n",
       "F9528      99\n",
       "F11440     98\n",
       "F10911     85\n",
       "F10761     80\n",
       "F10989     80\n",
       "F11013     77\n",
       "F9481      65\n",
       "F10991     64\n",
       "F13066     57\n",
       "F12822     53\n",
       "F10954     47\n",
       "F9219      46\n",
       "F9524      41\n",
       "F11231     36\n",
       "F10993     35\n",
       "F12987     35\n",
       "F10329     34\n",
       "F9132      34\n",
       "F11062     34\n",
       "F9658      32\n",
       "F11567     32\n",
       "F10262     30\n",
       "F8975      26\n",
       "F9419      24\n",
       "F10105     24\n",
       "F11557     24\n",
       "F12981     23\n",
       "F15250     22\n",
       "F4734      21\n",
       "F12984     21\n",
       "F12953     20\n",
       "F12684     20\n",
       "F9127      20\n",
       "F9130      20\n",
       "F15268     20\n",
       "F11018     20\n",
       "F12961     19\n",
       "F18217     19\n",
       "F13201     19\n",
       "F13203     19\n",
       "F11467     18\n",
       "F14477     18\n",
       "F8955      18\n",
       "F13132     17\n",
       "F8965      17\n",
       "F14266     16\n",
       "F10874     16\n",
       "F11402     15\n",
       "F12380     15\n",
       "F16612     15\n",
       "F9054      15\n",
       "F11107     15\n",
       "F14302     14\n",
       "F9890      14\n",
       "F4954      14\n",
       "F12540     14\n",
       "F14544     14\n",
       "F9055      13\n",
       "F17655     13\n",
       "F13299     13\n",
       "F14554     12\n",
       "F11067     12\n",
       "F7760      12\n",
       "F14225     11\n",
       "F11100     11\n",
       "F15961     11\n",
       "F16692     11\n",
       "F11401     11\n",
       "F16840     11\n",
       "F11498     11\n",
       "F17936     11\n",
       "F17915     10\n",
       "F9056      10\n",
       "F11687     10\n",
       "F4809      10\n",
       "F15582      9\n",
       "F14888      9\n",
       "F4199       9\n",
       "F8401       9\n",
       "F10002      9\n",
       "F12986      9\n",
       "F7033       8\n",
       "F16414      8\n",
       "F18973      8\n",
       "F15871      8\n",
       "F20175      8\n",
       "F8713       8\n",
       "F16125      8\n",
       "F14719      8\n",
       "F14728      8\n",
       "F15296      8\n",
       "F8582       8\n",
       "F9516       8\n",
       "F16120      7\n",
       "F16570      7\n",
       "F7036       7\n",
       "F7034       7\n",
       "F17009      7\n",
       "F4058       7\n",
       "F18141      7\n",
       "F12960      7\n",
       "F12346      7\n",
       "F14298      7\n",
       "F9653       7\n",
       "F14543      7\n",
       "F1529       6\n",
       "F14821      6\n",
       "F9526       6\n",
       "F16575      6\n",
       "F15293      6\n",
       "F9573       6\n",
       "F17448      6\n",
       "F17941      6\n",
       "F14933      6\n",
       "F17956      6\n",
       "F18034      6\n",
       "F2518       6\n",
       "F18247      6\n",
       "F19264      6\n",
       "F10467      6\n",
       "F14885      6\n",
       "F16943      6\n",
       "F19039      6\n",
       "F11359      6\n",
       "F11441      6\n",
       "F11096      6\n",
       "F11102      6\n",
       "F9840       5\n",
       "F4693       5\n",
       "F15639      5\n",
       "F4647       5\n",
       "F18017      5\n",
       "F18028      5\n",
       "F9037       5\n",
       "F4399       5\n",
       "F18075      5\n",
       "F18128      5\n",
       "F1416       5\n",
       "F3958       5\n",
       "F8935       5\n",
       "F11720      5\n",
       "F214        5\n",
       "F14235      5\n",
       "F15300      5\n",
       "F20066      5\n",
       "F18585      5\n",
       "F19116      5\n",
       "F19101      5\n",
       "F13051      5\n",
       "F11063      5\n",
       "F17348      5\n",
       "F16735      5\n",
       "F16308      5\n",
       "F16333      5\n",
       "F16071      5\n",
       "F7799       5\n",
       "F7768       5\n",
       "F13316      5\n",
       "F19051      5\n",
       "F15951      5\n",
       "F12873      5\n",
       "F16673      5\n",
       "F6611       5\n",
       "F6544       5\n",
       "F537        5\n",
       "F4818       5\n",
       "F4813       5\n",
       "F17054      5\n",
       "F6546       4\n",
       "F19539      4\n",
       "F474        4\n",
       "F11017      4\n",
       "F4817       4\n",
       "F19289      4\n",
       "F19385      4\n",
       "F12830      4\n",
       "F19581      4\n",
       "F4214       4\n",
       "F11127      4\n",
       "F8732       4\n",
       "F9112       4\n",
       "F10571      4\n",
       "F6608       4\n",
       "F12434      4\n",
       "F18713      4\n",
       "F19009      4\n",
       "F13581      4\n",
       "F16565      4\n",
       "F16550      4\n",
       "F16508      4\n",
       "F16501      4\n",
       "F16851      4\n",
       "F17124      4\n",
       "F17133      4\n",
       "F15605      4\n",
       "F12963      4\n",
       "F18131      4\n",
       "F18133      4\n",
       "F13296      4\n",
       "F1359       4\n",
       "F14812      4\n",
       "F14618      4\n",
       "F9236       3\n",
       "F12766      3\n",
       "F9983       3\n",
       "F16561      3\n",
       "F14470      3\n",
       "F7767       3\n",
       "F9858       3\n",
       "F14154      3\n",
       "F7963       3\n",
       "F16016      3\n",
       "F15849      3\n",
       "F885        3\n",
       "F15816      3\n",
       "F12443      3\n",
       "F15009      3\n",
       "F14702      3\n",
       "F14779      3\n",
       "F15660      3\n",
       "F10834      3\n",
       "F12265      3\n",
       "F6613       3\n",
       "F9515       3\n",
       "F12241      3\n",
       "F1547       3\n",
       "F13417      3\n",
       "F15000      3\n",
       "F13388      3\n",
       "F7302       3\n",
       "F9988       3\n",
       "F5624       3\n",
       "F18176      3\n",
       "F18138      3\n",
       "F19708      3\n",
       "F12959      3\n",
       "F18073      3\n",
       "F17943      3\n",
       "F1771       3\n",
       "F17071      3\n",
       "F2046       3\n",
       "F12832      3\n",
       "F18504      3\n",
       "F5938       3\n",
       "F18642      3\n",
       "F5843       3\n",
       "F18493      3\n",
       "F5903       3\n",
       "F19125      2\n",
       "F18258      2\n",
       "F14974      2\n",
       "F19213      2\n",
       "F9498       2\n",
       "F19437      2\n",
       "F19669      2\n",
       "F10491      2\n",
       "F18341      2\n",
       "F18423      2\n",
       "F19343      2\n",
       "F14783      2\n",
       "F18180      2\n",
       "F16689      2\n",
       "F9178       2\n",
       "F564        2\n",
       "F13069      2\n",
       "F14341      2\n",
       "F16515      2\n",
       "F7778       2\n",
       "F575        2\n",
       "F12816      2\n",
       "F1627       2\n",
       "F523        2\n",
       "F9152       2\n",
       "F12770      2\n",
       "F16913      2\n",
       "F12955      2\n",
       "F15523      2\n",
       "F10041      2\n",
       "F15297      2\n",
       "F18173      2\n",
       "F11644      2\n",
       "Name: duration, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of unique durations per fireid\n",
    "perimdf.groupby('fireid').duration.nunique().sort_values(ascending=False)\n",
    "# perimdf['duration'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf.drop(columns=['buffer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf['Ig_Date'] = perimdf['Ig_Date'].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf.to_file(pjoin(VIIRS_DIR, \"viirs_perims_shapefile.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the shapefile\n",
    "perimdfshape = gpd.read_file(pjoin(VIIRS_DIR, \"viirs_perims_shapefile.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf = open_vectors(pjoin(VIIRS_DIR, \"viirs_perims_shapefile.shp\")).data.to_crs(\"EPSG:5071\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perimdf to a tif file\n",
    "perimdf.to_file(pjoin(VIIRS_DIR, \"viirs_perims_geotiff.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tif\n",
    "perimdfgeotiff = Raster(pjoin(VIIRS_DIR, \"viirs_perims_geotiff.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_perims[2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf = gpd.read_parquet(PATHS[\"viirs_perim\"]).to_crs(\"EPSG:5071\")\n",
    "perimdf.drop(columns=[\"buffer\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a new DataFrame excluding the datetime column\n",
    "# df_without_datetime = perimdf[['geometry', 'duration', 'fireid']]\n",
    "\n",
    "# # Write this DataFrame to a shapefile\n",
    "# df_without_datetime.to_file(\"viirs_without_datetime.shp\")\n",
    "\n",
    "# # Create another DataFrame with only id and datetime columns\n",
    "# df_with_datetime = perimdf[['fireid', 't', 'duration']]\n",
    "\n",
    "# # Write this DataFrame to a CSV file\n",
    "# df_with_datetime.to_csv(\"viirs_with_datetime.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the shapefile and CSV file\n",
    "# df_without_datetime = gpd.read_file(\"viirs_without_datetime.shp\")\n",
    "# df_with_datetime = pd.read_csv(\"viirs_with_datetime.csv\")\n",
    "\n",
    "# # Merge the two DataFrames on 'fireid'\n",
    "# merged_df = df_without_datetime.merge(df_with_datetime, on=['fireid', 'duration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parquets from /home/jake/FireLab/Project/data/temp2/expanded  \n",
    "parq0 = dgpd.read_parquet('/home/jake/FireLab/Project/data/temp2/expanded/part.0.parquet')\n",
    "parq1 = dgpd.read_parquet('/home/jake/FireLab/Project/data/temp2/expanded/part.1.parquet')\n",
    "parq2 = dgpd.read_parquet('/home/jake/FireLab/Project/data/temp2/expanded/part.2.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parq0.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop buffer column\n",
    "perimdf = perimdf.drop(columns=['buffer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from perimdf2018, plot fireid F4734 at every duration step\n",
    "perimdf2018_f4734 = perimdf[perimdf.fireid == 'F4734']\n",
    "perimdf2018_f4734 = perimdf2018_f4734.sort_values(by=['duration'])\n",
    "\n",
    "duration_keys = perimdf2018_f4734.duration.unique()\n",
    "\n",
    "for i in range(len(duration_keys)):\n",
    "    # plot the fireid F4734 at each duration step\n",
    "    perimdf2018_f4734_duration = perimdf2018_f4734[perimdf2018_f4734.duration == duration_keys[i]]\n",
    "    perimdf2018_f4734_duration.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firelab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
